---
title : "$4-5. 학습 알고리즘 구현"
category :
    - mit
tag :
    - depp_learning
    - machine_learning
    - computer vision
toc : true
toc_sticky: true
comments: true
---
헉습 알고리즘을 구현해보자.


# 배운 내용 정리

본격적으로 학습 알고리즘을 구현해보기에 

앞서 여태까지 배운 내용들을 한 번 정리해보자.

-   전제 : 신경망에는 적응 가능한 **가중치와 편향**이 있고, 이 **가중치와 편향을** 훈련 데이터에 적응하도록 **조정하는 과정을 '학습'**이라고 한다.
-   1단계 - **미니배치** 
-   **훈련 데이터 중 일부를 무작위**로 가져온다. 
    >이렇게 **선별한 데이터를 미니배치**라 하며, <br/>
    그 미니배치의 **손실 함수 값을 줄이는 것이 목표**
-   2단계 - **기울기 산출**
      > 미니배치의 손실 함수 값을 줄이기 위해 **각 매중치 매개변수의 기울기**를 구한다. <br/> 
    > 기울기는 **손실함수의 값을 가장 작게 하는 방향을 제시**
-   3단계 - **매개변수 갱신**
    >   **가중치 매개변수를 기울기 방향으로 아주 조금 갱신**합니다.
-   4단계 - 반복
    >   1~3단계를 반복합니다.

신경망 학습은 위와 같은 순서로 이루어 진다. 

이것은 경사 하강법으로 매개변수를 갱신하는 방법이며, 

이때 미니배치로 무작위로 산정하기 때문에

확률적 경사 하강법 (Stochastic Gradient Descent, SGD) 라고 부른다. 

-   **확률적 경사 하강법** : '확률적으로 무작위로 골라낸 데이터'에 대해 수행하는 경사하강법

# 2층 신경망 구현

``` python
import sys, os
sys,path.append(os.pardir)
from common.functions import *
from common.gradient import numercial_gradient

class TwoLayerNet:
	def __init__(self, input_size, hidden_size, output_size, 
    	weight_init_Std=0.01):
		
        # 가중치와 편향 초기화
        # 가중치는 정규분포를 따르는 난수로, 편향은 0으로 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * \
        					np.random.randn(input_size, hidden_size)
		self.parpms['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * \
        					np.rnadom.randn(hideen_size, output_size)
        self.parpms['b2'] = np.zeros(output_size)
        
	def predict(self, x):
    	W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
    
    # x : 입력 데이터 ,t : 정답 레이블    
	def loss(self, x, t):
    	y = self.predict(x)
        
        return cross_entorpy_error(y, t)
     
    # 정확도를 구하는 함수 
    def acuuracy(self, x, t):
    	y= self.predict(x)
        # 원-핫 인코딩으로 되어있으므로 argmax를 통해 1의 위치를 찾아낸다.
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        # 맞춘 개수 / 전체 개수
        accuracy = np.sum(y==t) / float(x.shpae[0])
        return accuracy
        
 	# x : 입력 데이터, t: 정답 레이블
    def numercial_gradient(self, x, t):
    	loss_W = lamda W : self.loss(x, t)
        
        grad = {}
        grads['W1'] = numercial_gradient(loss_W, self.params['W1'])
        grads['b1'] = numercial_gradient(loss_W, self.params['b1'])
        grads['W2'] = numercial_gradient(loss_W, self.params['W2'])
        grads['b2'] = numercial_gradient(loss_W, self.params['b2])
        
        return grads
```

| 변수 | 설명 |
| --- | --- |
| params | 신경망의 매개변수를 보관하는 딕셔너리 변수 (인스턴스 변수)   params\['W1'\]은 1번째 층의 가중치, parms\['b1'\]은 1번째 층의 편향   params\['W2'\]은 2번째 층의 가중치, parms\['b2\]은 2번째 층의 편향 |
| grads | 기울기를 보관하는 딕셔너리 변수(numercial\_gradient() 메서드의 반환 값)   grads\['W1'\]은 1번째 층의 가중치의 기울기, grads\['b1'\]은 1번째 층의 편향의 기울기   grads\['W2'\]은 2번째 층의 가중치의 기울기, grads\['b2'\]은 2번째 층의 편향의 기울기 |

| 메서드 | 설명 |
| --- | --- |
| \_\_init\_\_(self.input\_size,   hidden\_size, output\_size) | 초기화를 수행한다.   인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수 |
| predict(self, x) | 예측(추론)을 수행한다.   인수 x는 이미지 데이터 |
| loss(self, x, t) | 손실 함수의 값을 구한다.   인수 x는 이미지 데이터, t는 정답레이블(아래 칸의 세 메서드의 인수들도 같음) |
| accuracy(self, x, t) | 정확도를 구한다. |
| numercial\_gradient(self, x, t) | 가중치 매개변수의 기울기를 구한다. |
| gradient(self, x, t) | 가중치 매개변수의 기울기를 구한다.   numercial\_gradient()의 성능 개선판 |

TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖는다.

paprams 변수에는 가중치 매개변수가 저장되는데, 예를 들어 1번째 층의 가중치 매개변수는

params\['W1'\]키에 넘파이 배열로 저장된다.

``` python
net = TwoLaterNet(input_size = 784, hidden_size = 100, output_size = 10)
net.params['W1'].shpae # (784, 100)
net.params['b1'].shaple # (100, )
net.parmas['W2'].shape # (100, 10)
net.params['b2'].shpae # (10, )
```

params 변수에는 이 신경망에 필요한 매개변수가 모두 저장된다.

그리고  params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용된다.

``` python
x = np.random.rand(100, 784) # 더미 입력 데이터 (100장 분량)
t = np.random.rand(100, 10) # 더미 정답 레이블 (100장 분량)

grads = net.numercial_gradient(x, t) # 기울기 계산

grads['W1'].shape #(784, 100)
grads['b1'].shpae #(100, )
grads['W2'].shape #(100, 10)
grads['b2'].shpae #(10, )
```

grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다. 

예를 들어 다음과 같이

numercial\_gradient() 메서드를 사용해 기울기를 계사낳면 grads 변수에 기울기 정보가 저장.

아직까지는 오차 역전파법을 배우지 않았기 때문에 간단하게 설명하자면,

numercial\_gradeint(self, x, t)는 수치 미분 방식으로 매개변수의 기울기를 계산한다.

이 기울기 계산을 고속으로 수행하는 기법이 바로 `오차역전파법` 이다.

오차역전파법을 쓰면 수치 미분을 사용할 때와 거의 같은 결과를 훨씬 빠르게 얻을 수 있습니다.

# 미니배치 학습 구현

 미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고 (미니배치),

그 미니배치에 대해서 경사하강법으로 매개변수를 갱신합니다. 

```
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = \
	load_mnist(normalize=True, one_hot_label=True)
    
 train_loss_list = []
 
 # 하이퍼 파라미터
 iters_num = 10000 # 반복 횟수
 train_size = x_train.shape[0]
 batch_size= 100 # 미니배치 크기
 learning_rate = 0.1
 
 network = TwoLayerNet(input_size= 784, hidden_size=50, output_size=10)
 
 for i in range(inters_num):
 	# 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    grad = network.numercial_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch) # 성능개선판
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
    	network.params[key] -= learning_rate * grad[key]
    	    
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
```

미니배치 크기는 100으로, 매번 60,000개의 훈련 데이터에서 

임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려낸다.

그리고 그 100개의 미니배치를 대상으로 
 
 확률적 경사 하강법을 수행해 매개변수를 갱신한다.

1. 경사하강법에 의한 갱신횟수를 10000번으로 설정하고, 

2. 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고,

3. 그값을 배열에 추가한다. 

# 시험 데이터 평가

 손실 함수의 값이란, 정확히는 '훈련 데이터의 미니배치에 대한 손실 함수'의 값이다.

훈련 데이터의 **손실 함수 값이 작아지는 것은 신경망이 잘 학습하고 있다는 방증**이지만, 

이 결과만으로는 다른 데이터셋에도 효과를 보여줄지는 불확실하다.

신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지 확인해야 한다.

다른 말로 '**오버피팅'**을 일으키지 않는지 확인해야 한다.

 **오버피팅이란 훈련 데이터에**

**포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻**.

**신경망 학습의 궁극적인 목표는 범용적인 능력**을 익히는 것이다.

**범용적인 능력을 평가하려면 훈련 데이터에 포함되지 않은 데이터를 사용해 평가**해봐야 한다.

1에폭별로 훈련 데이터와 시험 데이터에 대한 정확도를 

기록하도록 코드를 수정해보자.

``` python
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = \
	load_mnist(normalize=True, one_hot_label=True)
    

 
 # 하이퍼 파라미터
 iters_num = 10000 # 반복 횟수
 train_size = x_train.shape[0]
 batch_size= 100 # 미니배치 크기
 learning_rate = 0.1
 train_loss_list = []
 # <----- 새로 추가된 부분 -------->
 train_acc_list = []
 test_acc_list = []
 
 network = TwoLayerNet(input_size= 784, hidden_size=50, output_size=10)
 
 # <----- 새로 추가된 부분 -------->
 iter_per_epoch = max(train_size/ batch_size, 1)
 
 for i in range(inters_num):
 	# 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    grad = network.numercial_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch) # 성능개선판
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
    	network.params[key] -= learning_rate * grad[key]
    	    
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # <----- 새로 추가된 부분 -------->
    # 1에폭당 정확도 계산
    if i % iter_per_epoch == 0:
    	train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_text, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
```

1epoch 마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고,

그 결과를 기록합니다.

정확도를 1epoch 마다 계산하는 이유는 for 문 안에서 매번 계산하기에는

시간이 오래걸리고,

또 그렇게까지 자주 기록할 필요도 없기 때문이다.

위의 결과를 그래프로 그려보면

에폭이 진행될수록(학습이 진행될수록) 훈련 데이터와 시험 데이터를 사용하고

평가한 정확도가 모두 좋아지고 있다. 

또 두 선이 거의 겹쳐 있는것으로 보아 

두 정확도에는 거의 차이가 없음을 알 수 있다.

다시말해 오버피팅이 일어나지 않았음을 알 수 있다.

# 요약

-   수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 오차 역전파법을 이용하면 기울기를 고속으로 구할 수 있다.
-   신경망에는 적응 가능한 **가중치와 편향**이 있고, 이 **가중치와 편향을** 훈련 데이터에 적응하도록 **조정하는 과정을 '학습'** 이라고 한다.
-   **확률적 경사 하강법** : '확률적으로 무작위로 골라낸 데이터'에 대해 수행하는 경사하강법
-   **오버피팅이란 훈련 데이터에** **포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻**
-   **범용적인 능력을 평가하려면 훈련 데이터에 포함되지 않은 데이터를 사용해 평가해야 한다.**