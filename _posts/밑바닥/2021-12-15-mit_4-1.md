---
title : "$4-1. 신경망 학습 (손실함수)"
category :
    - mit
tag :
    - depp_learning
    - machine_learning
    - computer vision
toc : true
toc_sticky: true
comments: true
---
신경망 학습에 대하여 알아보자.

# 신경망 학습 

-    학습이란? 훈련 데이터로부터 가중치 매개변수의 최적 값을 자동으로 획득하는 것

신경망은 **손실 함수** 라는 지표를 통해 학습 가능하다. 

 이 장의 학습 목표는 **손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것**이다.

그 방법중 하나로 함수의 기울기를 활용할 수 있다.

 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.

만약 우리가 매개변수를 일일이 수작업으로 결정해야 한다면,

 이것은 엄청난 시간과 노력이 필요할 것이고, 

그 숫자가 늘어날수록 사실상 불가능에 가깝다고 볼 수 있다. 

 기계학습은 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다.

신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 

**사람의 개입을 더욱 배제**할 수 있게 해주는 중요한 특성을 지닌다.

 신경망을 배울 때 대표적으로 나오는 예시 중의 하나인 MNIST를 다뤄 보며 배워 보자.

MNIST는 손글씨 숫자를 인식하는 문제이다. 

우리는 이미지에서 **특징을 추출**하고 특징의 패턴을 기계학습 기술로 학습할 수 있다.

-    특징(feature) : 입력 데이터에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기 

 이미지의 특징은 보통 벡터로 기술하며 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고,

변환된 벡터를 지도 학습을 통해 학습할 수 있다. 

**기계학습**에서 규**칙을 찾아내는 역할은 "기계**"가 담당하지만, 

이미지를 **벡터로 변환할 때 사용하는 특징은 여전히 "사람"이 설계**하는 것에 주의

 반면 **신경망(딥러닝) 방식**은 사람이 개입하지 않도록 구성된다. 

즉, **특징을 찾아내는 역할 마저 "기계"가 스스로 학습**하는 것.

이러한 이유로 딥러닝을 종단간 기계학습 (**end-to-end** maching learning)이라고도 함

처음부터 끝까지, 입력에서 결과를 사람의 개입 없이 얻는다는 뜻을 담고 있음. 

 신경망을 설명하기 앞서, 기계학습에서 데이터를 취급할때 주의할 점을 짚어 보자.

기계학습에서는 일반적으로 데이터를

**훈련데이터(training data)**와 **시험데이터(test data)**로 나눠 학습과 실험을 수행한다. 

우선 훈련데이터를 통해 학습하며 최적의 매개변수를 찾은 뒤 시험 데이터를 사용하여

훈련한 모델의 성능을 평가한다.

왜 꼭 이렇게 나눠야 할까?

그 이유는 바로 **범용 능력을 제대로 평가**하기 위해서 이다!

-   범용 능력 : 아직 보지 못한 데이터로도 문제를 올바르게 풀어내는 능력 

# 손실 함수

 신경망은 **'하나의 지표'를 기준으로 최적의 매개변수 값을 탐색**한다. 

신경망 학습에서 사용하는 지표는 **손실 함수(loss function)**라고 한다. 

이 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 **"오차 제곱합"과** **"교차 엔트로피 오차"**를 사용한다.

손실 함수는 신경망 성능의 "나쁨"을 나타내는 지표로 현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 "못"하느냐를 나타낸다. 

 가장 많이 쓰이는 손실 함수는 오차 제곱합(Sum of squares for error, SSE)이다.

![](/assets/image/2022-02-16-17-55-05.png)

y는 신경망의 출력(신경망이 추정한 값)

t는 정답 레이블

k는 데이터의 차원수

를 의미한다.

즉 오차(추정-정답)를 제곱한 값을 모두 더한 뒤 2로 나눠준 값이 오차 제곱합이다.

(여기서 2로 나눠주는 이유는 t가 원-핫 인코딩으로 표현되어 있고, 

오차가 가장 클 때 1, 가장 작을 때 -1로

크기 차이가 2가 되기 때문에 정규화해주기 위하여 2로 나누는 것.

 만약 t가 원-핫 인코딩이 아니라면

전체 데이터의 개수인 n으로 나누어 주는 것도 올바른 표기이다)

**원-핫 인코딩**이란 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법을 의미.

각 숫자(0~9)를 나타낼 확률을 y, t는 정답 레이블을 원-핫 인코딩으로 표기해 보자.

```
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

0을 나타낼 확률 = 0.1

1을 나타낼 확률 = 0.05

2를 나타낼 확률 = 0.6... 이므로 추정한 값은 확률이 가장 큰 2가 되고 정답 레이블을 보면 2가 정답임을 알 수 있다.

앞서 봤던 오차 제곱합 수식을 코드로 옮겨 보자.

```
def sum_squares_error(y, t):
	return 0.5 * np.sum((y-t)**2)
```

파라미터 y와 t는 넘 파이 배열이다.

```python
# 정답은 '2'
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 예1: '2'일 확률이 가장 높다고 추정(0.6)
y= [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
0.097500000000031

# 예2: '7'일 확률이 가장 높다고 추정(0.6)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))
0.597500000000003
```

첫 번째 예는 추정 값 2, 정답 2, 오차 0.0975...

두 번째 예는 추정 값 7, 정답 2 오차 0.5975... 

첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 적은 것을 알 수 있다.

즉, 오차 제곱합 기준으로는 첫 번째 추정 결과가 오차가 더 작기 때문에 정답에 더 가까울 것으로 판단 가능.

# 교차 엔트로피 오차 

 **교차 엔트로피 오차(Cross entropy error, CEE)** 역시 손실 함수로서 자주 이용한다. 

![](/assets/image/2022-02-16-17-55-33.png)

log는 밑이 e인 자연로그를 의미한다.

y는 신경망의 출력

t는 정답 레이블(원-핫 인코딩)

 원-핫 인코딩 특성상 **정답인 인덱스의 원소만 1**이 되므로 나머지는 모두 0이 된다. 

따라서 실질적으로 정답일 때의 추정의 자연로그를 계산하는 식이 된다.

(정답이 아닌 나머지 모두는 t가 0이므로 logy와 곱해도 0이 되어 결과에 영향을 주지 않다.)

즉, **교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.**

![](/assets/image/2022-02-16-17-55-55.png)

 위 그림에서 보듯 x가 1일 때 y는 0이 되고 x가 0에 가까워질수록 y의 값은 점점 작아진다. 

교차 엔트로피 오차도 마찬가지로 정답에 해당하는 출력이 커질수록 0에 다가가다가 그 출력이 1일 때 0이 된다.

반대로 정답일 때의 출력이 작아질수록 오차는 커진다.

```python
def cross_entropy_error(y, t):
	delta = 1e-7
    return -np.sum(t*np.log(y+delta))
```

 y와 t는 넘 파이 배열이다.

delta를 더해준 이유는 np.log() 함수에 **0을 입력하면 마이너스 무한대 (-inf)가 되어 계산이 불가**하기 때문에 

아주 작은 값을 더해서 절대 0이 되지 않도록 트릭을 써준 것.

```python
t = [0, 0, 1, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
0.5108254570

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
2.3025849029
```

첫 번째 예 : 정답 출력 0.6, 교차 엔트로피 오차 0.51

두 번째 예 : 정답 출력 0.1 교차 엔트로피 오차 2.3

즉, 오차가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한 것으로 앞서 오차 제곱 합의 판단과 일치.

# 요약

-   기계학습에서 사용하는 데이터셋은 **훈련 데이터**와 **시험 데이터**로 나눠 사용한다.
-   훈련 데이터로 학습한 모델의 **범용 능력을 시험 데이터로 평가**한다.
-   신경망 학습은 **손실 함수**를 지표로, **손실 함수의 값이 작아지는 방향**으로 가중치 매개변수를 갱신한다.
-   손실 함수로는 대표적으로 **오차제곱합(SSE)**과 **교차 엔트로피 오차(CEE)**를 사용한다.
-   오차제곱합은 정답과 오답의 **모든 확률을 고려**하고, 교차 엔트로피 오차는 **정답의 확률만을 고려**한다.