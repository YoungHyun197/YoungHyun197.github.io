---
title : "$5-3. 활성함수와 오차역전파 구현"
category :
    - mit
tag :
    - depp_learning
    - machine_learning
    - computer vision
toc : true
toc_sticky: true
comments: true
---
활성함수와 오차역전파를 구현해보자.

# ReLU 계층
```python
class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```
Relu 계층은 mask 라는 인스턴스 변수를 가진다.

mask 는 True/False 로 구성된 넘파일 배열로,

순전파의 입력인 x 원소 값이 0이하인 인덱스는 True,

그 외 0보다 큰 원소는 False 로 유지한다.

Relu 계층은 전기 회로의 스위치에 비유할 수 있다.

순전파 때 전류가 흐르고 있으면 스위치를 On 으로 하고,

흐르지 않으면 Off 로 한다.

역전파 때는 스위치가 On 이라면 전류가 그대로 흐르고,

Off 이면 더 이상 흐르지 않는다.

# Sigmoid 계층
시그모이드 함수.

![](/assets/image/2022-02-20-19-57-07.png)



이를 계산 그래프로 그리면 다음과 같다.

![](/assets/image/2022-02-20-19-54-57.png)


exp와 / 노드가 새롭게 등장했다.
exp 노드는 y=exp(x) 계산을 수행하고
/ 노드는 y=1/x 계산을 수행한다.

이제 그림 5-19의 역전파를 알아보자.

1단계
/노드를 미분하면 다음 식이 된다.

![](/assets/image/2022-02-20-19-57-22.png)



계산 그래프에서는 다음과 같다.

![](/assets/image/2022-02-20-19-57-39.png)

2단계.
`+` 노드는 상류의 값을 여과 없이 하류로 내보다는 게 다이다.
계산 그래프에서는 다음과 같다.

![](/assets/image/2022-02-20-19-57-53.png)

3단계.
exp 노드의 미분은 다음과 같다.


![](/assets/image/2022-02-20-19-58-05.png)

계산 그래프는 다음과 같다.

![](/assets/image/2022-02-20-19-58-16.png)

4단계.
x 노드는 순전파 때의 값을 서로 바꿔 곱한다.

이상으로 위 그림처럼
Sigmoid 계층의 역전파를 계산 그래프로 완성했다.

![](/assets/image/2022-02-20-19-58-30.png)



 계산 그래프의 중간 과정을 모두 묶어 단순한 sigmoid 노드 하나로 대체할 수 있다.

![](/assets/image/2022-02-20-19-58-56.png)



계산 그래프의 간소화 버전은 역전파 과정의 중간 계산들을 생략할 수 있어 더 효율적인 계산이라 할 수 있다.
또 노드를 그룹화하여 Sigmoid 계층의 세세한 내용을 노출하지 않고
입력과 출력에만 집중할 수 있다는 것도 중요한 포인트이다.

또한 식을 다음과 같이 정리할 수 있다.

![](/assets/image/2022-02-20-19-59-23.png)


이처럼 Sigmoid 계층의 역전파는 순전파의 출력 y 만으로 계산할 수 있다.

![](/assets/image/2022-02-20-19-59-32.png)


Sigmoid 계층을 코드로 구현해보자.
``` python
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = sigmoid(x)
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out

        return dx
```
구현에서는 순전파의 출력을 인스턴스 변수 out 에 보관했다가,

역전파 계산 때 그 값을 사용한다.

![](/assets/image/2022-02-20-19-59-43.png)



# Affine 계층

```python 
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
        
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b
        return out
    
    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis = 0)
        return dx
```


# Softmax-with-Loss 계층

``` python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None  # loss
        self.y = None # softmax의 출력
        self.t = None # 정답 레이블(원-핫 벡터)
        
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss
    
    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

# 오차역전파법을 적용한 신경망 구현

2층의 레이어를 가지는 네트워크를 만들어본다. Gradient 메소드에서는 

`backpropagation`을 통해서 기울기값을 반환한다.

``` python
import sys, os
sys.path.append('./deep-learning-from-scratch-master')
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
        #계층 생성
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()
        
    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x
    
    # x : 입력데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: # 1이면 one-hot-lable이 아니고, 1이 아니면 one-hot-lable
            t = np.argmax(t, axis = 1)
            
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads
    
    def gradient(self, x, t):
        # 순전파
        self.loss(x, t) # 이 함수를 실행함으로서 self.lastLayer의 forward를 실행함.
        
        # 역전파
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        layers = list(self.layers.values()) #layers라는 oredered dict의 value값을 뽑음
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
            
        #결과 저장
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db
        
        return grads
```



수치미분으로 구한 기울기와 역전파법으로 구한 기울기 비교
아래코드에서는 수치 미분으로 구한 기울기와 역전파법으로 구한 기울기의 차이가 얼마 안나는것을 보인다.

``` python
import sys, os
sys.path.append('./deep-learning-from-scratch-master')
import numpy as np
from dataset.mnist import load_mnist
from ch05.two_layer_net import TwoLayerNet

# 데이터를 불러옴
(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)

# 네트워크 선언 및 초기화
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 미니배치로 0, 1, 2의 데이터만 불러옴
x_batch = x_train[:3]
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch) #수치미분에 의한 기울기
grad_backprop = network.gradient(x_batch, t_batch) # 역전파에 의한 기울기

for key in grad_numerical.keys():
    diff = np.average(np.abs(grad_backprop[key]-grad_numerical[key]))
    print(key+":"+str(diff))

#----------output-----------#
# 각 방법으로 구한 기울기의 차를 출력
# W1:3.771145507353469e-10
# b1:2.3007621810804038e-09
# W2:4.787288764258565e-09
# b2:1.3988587680979768e-07
```



오차역전파법을 사용한 학습 구현

Mnist데이터를 예측하고, 파라미터를 backpropagation을 통해 파라미터를 학습

``` python
import sys, os
sys.path.append('./deep-learning-from-scratch-master')
import numpy as np
from dataset.mnist import load_mnist
from ch05.two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size/batch_size, 1)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    #오차역전파법으로 기울기를 구한다.
    grad = network.gradient(x_batch, t_batch)
    
    #갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        train_acc_list.append(test_acc)
        print(train_acc, test_acc)
```