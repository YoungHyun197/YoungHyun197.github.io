---
title : "$4-4. 경사하강법"
category :
    - mit
tag :
    - depp_learning
    - machine_learning
    - computer vision
toc : true
toc_sticky: true
comments: true
---

경사하강법에 대하여 알아보자.



# 경사하강법

이전 시간에서는 x0와 x1의 편미분을 변수별로 따로 계산했다.

x0와 x1의 편미분을 동시에 계산하고 싶다면 어떻게 할까?

![](/assets/image/2022-02-16-19-01-07.png)


위처럼 양쪽의 편미분을 묶어서 벡터로 정리한 것을 기울기(gradient)라고 한다. 

기울기는 아래 코드와 같이 구현할 수 있다.

``` python
def numercial_gradient(f, x):
	h = 1e-4 # 0.0001
    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성
    
    for idx in range(x.size):
    	tmp_val = x[idx]
        # f(x+h) 계산
        x[idx]=tmp_val+h
        fxh1 = f(x)
        
        # f(x-h) 계산
        x[idx] = tmp_val-h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 값 복원
	
    return grad
```

변수가 두 개이더라도, 동작 방식은 변수가 하나일 때의 수치 미분과 거의 같다.

참고로, np.zeros\_like(x)는 x와 형상이 같고 그 원소가 모두 0인 배열을 만든다.

세 점 (3, 4), (0, 2), (3, 0)에서의 기울기를 구해보자.

``` python
numercial_gradient(function_2, np.array([3.0, 4.0]))
array([6., 8.]) *

numercial_gradient(function_2, np.array([0.0, 2.0]))
array([0., 4.]) 

numercial_gradient(function_2, np.array([3.0, 0.0]))
array([6., 0.])
```

이처럼 (x0, x1)의 각 점에서의 기울기를 계산할 수 있다. 

이 기울기라는게 의미하는 건 뭘까?

기울기 그림은 방향을 가진 벡터(화살표)로 그려진다.

기울기는 함수의 '가장 낮은 장소(최솟값)'을 가리키는 것 같다.

마치 나침반 처럼 화살표들은 한 점을 향하고, '가장 낮은 곳'에서 멀어질수록

화살표의 크기가 커짐을 알 수 있다.

![](/assets/image/2022-02-16-19-02-02.png)

사실 기울기는 각 지점에서 낮아지는 방향을 가리킨다.

정확히 말하자면 **기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을**

**가장 크게 줄이는 방향** 이다.




기계학습에서는 손실 함수가 최솟값이 될 때의 매개변수 값을 찾아내는 것이 목적이다.

일반적으로 현실에서 손실 함수는 매우 복잡하므로, 매개변수 공간이 광대하여 어디가

최솟값이 되는지 짐작할 수 없다.

이런 상황에서 **기울기를 잘 이용해 함수의 최솟값(또는 가능한 작은 값)을**

**찾으려는 것이 경사 하강법이다.**

그러나 주의할 것은 기울기가 가리키는 곳에 

정말 함수의 최솟값이 있는지는 보장할 수 없다.

실제로 복잡한 함수에서는 기울기가 가리키는 방향에 

최솟값이 없는 경우가 대부분이다.

안장점을 떠올려보면 위 말이 무엇을 의미하는지 쉽게 깨달을 수 있다.

![](/assets/image/2022-02-16-19-03-45.png)

기울기 방향이 꼭 최솟값을 가리키는 것은 아니나, 

그 방향으로 가야 함수의 값을 줄일 수 있다.

그래서 최솟값이 되는 장소를 찾는 문제에서는 

기울기 정보를 단서로 나아갈 방향을 정해야 한다.

경사 하강법은 **기울어진 방향으로 일정 거리만큼 이동 후**

**다음 이동한 곳에서도 마찬가지로 기울기를**

**구하고 기울어진 방향으로 나아가기를 반복**한다.

이렇게 해서 함수의 값을 점차 줄이는 것이 경사 하강법이다.

경사 하강법을 수식으로 나타내면 아래와 같습니다.

![](/assets/image/2022-02-16-19-04-31.png)

기호  **η는 에타**라고 읽으며 **갱신하는 양**을 나타낸다.

이를 신경망 학습에서는 **학습률(learning rate)**라고 한다.

한 번의 학습으로 얼마만큼 학습해야 할지, 

즉 매개변수 값을 얼마나 경신하느냐를 정하는 것이 학습률이다.

경사 하강법은 다음과 같이 간단하게 구현할 수 있다.

``` python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
	x = init_x
    
    for i in range(step_num):
    	grad = numercial_gradient(f, x)
        x -= lr * grad
        
    return x
```

f는 최적화하려는 함수, init\_x는 초깃값, lr은 learning rate (=학습률) step\_num은 반복 횟수를 뜻합니다.

함수의 기울기는 numercial\_gradient(f, x)로 구하고,

그 기울기에 학습률을 곱한 값으로 갱신하는 처리를 step\_num번 반복합니다.

``` python
# 문제: 경사하강법으로 f(x0, x1) = x0^2 + x1^2의 최솟값을 구하라.
def function_2(x):
	return x[0]**2 + x[1]**2
    
init_x = np.array([-3.0, 4.0])
gradient_descent(function_2, init_x=inti_x, lr=0.1, step_num=100)
array([-6.11110793e-10,   8.14814391e-10])
```

초깃값을 (-3.0, 4.0)으로 설정한 후 경사 하강법을 사용해 최솟값 탐색을 시작한다.

최종 결과는 (-6.1e-10, 8.1e-10)으로 거의 (0, 0)에 가까운 결과이다.

실제로 진정한 최솟값은 (0, 0) 이므로 경사 하강법으로 거의 정확한 결과를 얻었다.

해당 과정을 그림으로 나타내면 아래와 같다.

![](/assets/image/2022-02-16-19-06-58.png)

```
# 학습률이 너무 큰 예 : lr = 10.0
init_x = np.array([-3.0, 4,0])
grdient_descent(function_2, init_x=init_x, lr=10.0, step_num = 100)
array([-2.58983747e+13,   -1.29524862e+12])

# 학습률이 너무 작은 예 : lr = 1e-10
init_x = np.array([-3.0, 4,0])
grdient_descent(function_2, init_x=init_x, lr=1e-10, step_num = 100)
array([-2.9999999994,   3.9999999992])
```

**학습률이 너무 크면 큰 값으로 발산**해버리고, 

반대로 **너무 작으면 거의 갱신되지 않은 채 끝나**버린다.

[##_Image|kage@DumvA/btraJnZCMe9/zts0tXSmkf6LqGlhB4pkIk/img.png|CDM|1.3|{"originWidth":744,"originHeight":374,"style":"alignCenter","caption":"학습률이 너무 작은 경우와 학습률이 너무 큰 경우"}_##]

-   하이퍼 파라미터 : 학습률 같은 사람이 직접 설정해야 하는 매개변수 

![](/assets/image/2022-02-16-19-08-01.png)

W는 형상이 2 x 3인 가중치, L은 형상이 2 x 3인 손실 함수를 의미한다.

경사는 각각의 원소에 관한 편미분이다. 

여기서 중요한 점은 aL/aW의 형상이 W와 같다는 것이다.

실제로 W와 aL/aW 형상 모두 2 x 3 이다.

``` python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numercial_gradient

class simpleNet:
	def __init__(self):
    	self.W = np.random.randn(2, 3) # 정규분포로 초기화
        
	def perdeict(self, x):
		return np.dot(x, self.W)
        
	def loss(self, x, t):
		z = self.predict(x)
		y = softmax(z)
		loss = cross_entropy_error(y, t)

		return loss
```

simpleNet class는 형상이 2x3 인 가중치 매개변수 하나를  인스턴스 변수로 갖는다.

메서드는 2개인데 하나는 예측을 수행하는 predict(x)이고, 

다른 하나는 손실 함수의 값을 구하는 loss(x, t) 이다.

여기에서 인수 x는 입력 데이터, t는 정답 레이블이다.

``` python
net = simpleNet()
print(net.W) # 가중치 매개변수
[[ 0.47355232,   0.9977393,    0.84668094]
 [ 0.85557411,   0.03563661,   0.69422093] ]
 
 x = np.array([0.6, 0.9])
 p = net.predict(x)
 print(p)
 [1.05414809 0.63071653, 1.1328074]
 np.argmax(p) # 최댓값의 인덱스
 2
 
 
 t = np.array([0, 0, 1])
 net.loss(x, t)
 0.92806853663411326
 
 
 
def f(W):
	return net.loss(x,t)
   

dW = numercial_gradient(f, net.W)
print(dW)
[[ 0.21924763 0.14356247 -0.36281009]
 [ 0.32887144 0.2153437  -0.54421514]]
```

f(W) 함수의 인수 W는 더미로 만든 것이다. 

numercial\_gradient(f, x) 내부에서 f(x)를 실행하는데 그와의

일관성을 위해 f(W)를 정의한 것이다.

numercial\_gradient(f, x)의 인수 f는 함수, x는 함수 f의 인수이다. 

그래서 net.W를 인수로 받아 손실 함수를 계산하는 새로운 f를 정의하였다. 

그리고 이 새로 정의한 함수를 numercial\_gradient(f, x)에 넘긴다.

참고로 파이썬에서는 def로 정의하기보다는 람다(lamda) 기법을 쓰면 더 편하다.

``` python
f = lamda w : net.loss(x, t) # lamda 다음에 인자를 받고 : 이후에 정의
dW = numercial_gradient(f, net.W)
```

신경망의 기울기를 구한 다음에는 경사 법에 따라 가중치 매개변수를 갱신하기만 하면 된다.

# 요약

-   **기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장** **크게 줄이는 방향입니다.**
-   **η는 에타**라고 읽으며 **갱신하는 양**을 나타내고, 이를 신경망 학습에서는 **학습률(learning rate)**라고 한다.
-   **학습률이 너무 크면 큰 값으로 발산**해버리고, 반대로 **너무 작으면 거의 갱신되지 않은 채 끝나**버린다.
-   **하이퍼 파라미터** : 학습률 같은 사람이 직접 설정해야 하는 매개변수