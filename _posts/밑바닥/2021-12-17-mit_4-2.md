---
title : "$4-2. 신경망 학습 (미니배치)"
category :
    - mit
tag :
    - depp_learning
    - machine_learning
    - computer vision
toc : true
toc_sticky: true
comments: true
---

미니 배치 학습에 대하여 알아보자

# 미니배치


기계학습에서는 훈련 데이터에 대한 손실 함수의 값을 구하고, 

그 값을 최대한 줄여주는 매개변수를 찾아낸다.

이렇게 하기 위해서는 모든 훈련 데이터를 대상으로 손실 함숫값을 구해야 한다.

이전 시간에는 데이터 하나에 대한 손실 함수만 고려하였고,

오늘은 **훈련 데이터 모두에 대한 손실 함수의** **합을 구하는 방법**을 생각해 보자

![](/assets/image/2022-02-16-18-04-58.png)


 데이터가 N개라면 tnk는 n번째 데이터의 k번째 값을 의미한다. 

(ynk는 신경망의 출력, tnk는 정답 레이블.)

마지막에 N으로 나누어 정규화하고 있다. 

N으로 나눔으로써  **"평균 손실 함수"를** 구하는 것이다.

이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 통일된 지표를 얻을 수 있다.

 하지만 모든 데이터를 대상으로 손실함수의 합을 구하기는 것은 

오래 걸릴 뿐 아니라 비현실적이기 때문에

 데이터 일부를 추려 전체의 "근사치"로 이용가능. 

이 일부를 우리는 **"미니 배치"** 라고 부른다. 

가령 MNIST의 6만 개의 훈련 데이터 중에서

100장을 무작위로 뽑아 그 100장만을 사용하여 학습하는 것.

이러한 학습 방법을 **"미니 배치 학습"이라고** 한다.

 미니배치 학습을 구현하는, 즉 훈련 데이터에서 지정한 수의 데이터를

무작위로 골라내는 코드를 작성해 보자.

``` python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, y_test) = \
	load_mnist(normalize=True, one_hot_label=True)
    
print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)
```

 load\_mnist 함수는 MNIST 데이터 셋을 읽어오는 함수이다.

이 함수는 dataset/mnist.py 파일에 있다.

이 함수는 훈련 데이터와 시험 데이터를 읽는다.

호출할 떄 one\_hot\_labe=Ture로 지정하여 원-핫 인코딩으로, 

즉 정답 위치의 원소만 1이고 나머지가 0인 배열을 얻을 수 있다.

이 훈련 데이터에서 무작위로 10장만 빼내려면 어떻게 하면 될까?

넘 파이의 np.random.choice() 함수를 쓰면 다음과 같이 간단히 해결 가능하다. 

``` python
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

# np.random.choice(60000, 10) 은 
# 0 이상 60000 미만의 수 중에서 무작위로 10개를 골라낸다.
```

 이제 무작위로 선택한 이 인덱스를 사용해 미니배치를 뽑아내기만 하면 된다.

손실 함수도 이 미니배치로 계산한다.

``` python
def cross_entropy(y, t):
	if y.dim == 1:
    	t = t.reshpae(1, t.size)
    	y = y.reshape(1. y.size)
    
    batch_size = y.shpae[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

 y는 신경망의 출력, t는 정답 레이블을 의미합니다. y가 1차원이라면,

즉 데이터 하나당 교차 엔트로피 오차를구하는 경우는 

reshape 함수로 데이터의 형상을 바꿔준다.

그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 

평균의 교차 엔트로피 오차를 계산한다.

정답 레이블이 원-핫 인코딩이 아니라 숫자 레이블로 주어졌을 때의 

교차 엔트로피 오차는 아래와 같다.

``` python
def cross_entropy(y, t):
	if y.dim == 1:
    	t = t.reshpae(1, t.size)
    	y = y.reshape(1. y.size)
    
    batch_size = y.shpae[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

원-핫 인코딩일 떄 t가 0인 원소는 교차 엔트로피 오차도 0이므로,

 그 계산은 무시해도 좋다는 것이 핵심.

다시 말하면 정답에 해당하는 신경망의 출력만으로 

교차 엔트로피 오차를 계산할 수 있다.

np.log(y \[np.arrange(batch\_size,) t\]) 부분을 자세히 살펴보면

(batch\_size)는 0부터 batch\_size -1까지 배열을 생성합니다. 

즉, batch\_size가 5이면 np.arrange(batch\_size)는 \[0, 1, 2, 3, 4\]라는 넘 파이 배열을 생성한다.

t에는 레이블이 \[2, 7, 0, 9, 4\]와 같이 저장되어 있으므로 y \[np.arrange(bath\_size), t\]는

각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다. 

이 예에서는 \[y \[0,2\], y \[1,7\], y \[2,0\], y \[3,9\], y \[4,4\]\]인 넘 파이 배열을 생성한다.

그렇다면 "정확도"라는 지표를 나 두고 **"손실 함수"는 굳이 왜 사용하는 걸까?** 

그것은 바로 **"미분"의 역할에 주목**하면 알 수 있다.

신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 

값을 가능한 작게 하는 매개변수 값을 찾는다.

이때  매개변수의 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 

서서히 경신하는 과정을  반복한다.

만약 미분 값이 음수면 가중치 매개변수를 양의 방향으로 변화시켜 

손실 함숫값을 줄일 수 있다. 

그러나 **미분 값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않는다.**

그래서 가중치 매개변수의 갱신은 거기서 멈추게 된다.

-   신경망이 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 된다. 

그렇다면 **정확도를 지표로 삼으면 매개변수의 미분이 대부분의 장소에서 0이 되는 이유**는 무엇일까?

정확도를 지표로 삼으면 매개변수를 약 만만 조정해서는 정확도가 개선되지 않고 일정하게 유지된다.

정확도가 개선된다 하더라도 그 값은 불연속적인 값으로 바뀌어버림. 

하지만, **손실 함수를 지표로 삼으면 0.9124.. 와 같은 수치로 나타나고 매개변수의 값이 조금 변하면**

그에 반응하여 손실 함수의 값도 0.9125...처럼 **연속적으로 변화**한다.

**정확도는 매개변수의 미세한 변화에는 거의 반응을 보이지 않고, 반응이 있더라고 그 값이 불연속적**으로

갑자기 변화합니다. 이것은 **"계단 함수"를 활성화 함수로 사용하지 않는 이유**와도 같다. 

만약 활성화 함수로 계단 함수를 이용하면 대부분의 장소에서 0이 나오기 때문에 

손실 함수를 지표로 삼는 게 아무 의미가 없게 된다. 

매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여

손실 함수의 값에는 아무런 변화가 나타나지 않기 때문. 

계단 함수는 한순간만 변화를 일으키지만, 시그모이드 함수의 미분은

출력이 연속적으로 변하고, 기울기도 연속적으로 변한다.

즉, 시그모이드 함수의 미분은 어느 장소라도 0이 되지는 않는다.

# 요약 

-   가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 경신하는 작업을 반복한다.
-   데이터 일부를 추리는 것을 **미니 배치라고** 하며, 그 데이터만을 사용하여 학습하는 방법을 **미니 배치 학습**이라 한다.
-   배치 학습에서는 손실 함수의 값을 모두 더하고 총 배치 사이즈로 나누어 주어야 한다.
-   정확도를 지표로 삼지 않는 이유는 대부분의 장소에서 미분이 0이 되기 때문이다. 
-   손실 함수를 지표로 삼으면 매개변수의 값의 미세한 변화에 따라 손실 함수의 값이 연속적으로 변화한다.
-   계단 함수를 활성화 함수로 사용하지 않는 이유는 정확도를 지표로 삼지 않는 이유와 같다
-   계단 함수 대신에 시그모이드 함수를 많이 사용한다. 이를 통해 출력과 기울기가 연속적으로 변하므로 어느 장소라도 0이 되지 않는다.