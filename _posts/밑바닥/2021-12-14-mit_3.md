---
title : "$3. 신경망(Nerual Network)"
category :
    - mit
tag :
    - depp_learning
    - machine_learning
    - computer vision
toc : true
toc_sticky: true
comments: true
---
신경망에 대하여 알아보자


# 3.1 퍼셉트론에서 신경망으로


## 3.1.1 신경망의 예


신경망은 복잡한 함수도 표현할 수 있다는 장점을 가지고 있지만,

각각의 가중치 설정은 사람이 수작업을 해주어야 한다는 단점이 존재한다.


![](/assets/image/2022-02-16-19-45-49.png)


신경망 그림은 위와 같다.

가장 왼쪽을 입력층, 중간을 은닉층, 가장 오른쪽을 출력층이라고 부른다.

중간층은 사람에게 보이지 않기 때문에 은닉층이라 부른다.

일반적으로 왼쪽(입력층)부터 0층 1층 2층이라고 부른다.


위 그림만 보면 앞서 배운 퍼셉트론과 큰 차이가 없다.


## 3.1.2 퍼셉트론 복습

![](/assets/image/2022-02-16-19-46-15.png)

y = { 0 (b + w1x1 + w2x2 <= 0 )

{ 1 (b + w1x1 + w2x2 > 0 )

여기서 b는 편향으로 뉴런이 얼마나 쉽게 활성화 되느냐를 제어하고,

w1과 w2는 각 신호의 가중치를 나타내는 매개변수이다.




대표사진 삭제
편향 명시 퍼셉트론

편향을 명시하면 위와 같은 그림으로 퍼셉트론을 나타낼 수 있다.

위 그림에는 가중치가 b이고 입력이 1인 뉴런이 추가되었다.

x1, x2, 1이라는 3개의 신호가 뉴런에 입력되어, 

각 신호에 가중치를 곱한 후 다음 뉴런에 전달된다.

다음 뉴런에서는 이 신호들의 값을 더한후 0을 넘으면 1을 출력하고 

그렇지 않으면 0을 출력한다.


편향은 항상 입력 신호가 1인것에 주의하자.


이것을 식으로 보다 간결하게 나타내면 아래와 같이 나타낼 수 있다.


y = h(b + w1x1 +w2x2)

h(x) = { 0 (x<=0)

{ 1 (x>0)

입력 신호의 총합이 h(x)라는 함수를 거쳐 변환되어, 

그 변환된 값이 y의 출력이 된다.

h(x)함수는 입력이 0을 넘으면 1을, 그렇지 않으면 0을 반환한다.



## 3.1.3 활성화 함수의 등장

이처럼 입력 신호의 총합을 출력 신호로 변환하는 함수를 활성화함수(activation function) 라고 한다.

식을 다시 정리해자.


a = b + w1x1 + w2x2

y = h(a)


뉴런을 큰 원(O)으로 그려보면 아래 그림처럼 나타난다.



![](/assets/image/2022-02-16-19-46-59.png)

기존 뉴런의 원을 키우고, 그 안에 활성화 함수의 처리 과정을 명시적으로 그려넣었다.

즉, 가중치 신호를 조합한 결과가 a라는 노드가 되고, 

활성화 함수 h()를 통과하여 y라는 노드로 변환되는 과정이다.



# 3.2 활성화 함수


임계값을 경계로 출력이 바뀌는 함수를 계단 함수(step function)이라 한다.

"퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다"


## 3.2.1 시그모이드 함수(sigmoid function)

![](/assets/image/2022-02-16-19-47-20.png)

신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고,

그 변환된 신호를 다음 뉴런에 전달한다.


퍼셉트론과 신경망의 주된 차이는 활성화 함수이다. 

(퍼셉트론 : 계단함수, 신경망 : 시그모이드)


## 3.2.2 계단 함수 구현



```python
def step_function(x):
    if x>0:
        return 1
    else:
        return 0

def step_function(x):
    y = x > 0
    return y.astype(np.int)

```    

처음에 만든 함수는 실수형만 입력을 받을수 있고 numpy 배열은 입력 받지 못한다.

따라서 두 번째 함수처럼 구현하면 numpy와 실수 모두 입력받을 수 있게 된다.


이것은 넘파이의 트릭을 이용한 원리로, x라는 넘파이 배열을 준비하고

y = x > 0을 인터프리터에 입력하게 되면

각각의 원소가 0보다 크면 True로, 

아니면 Fasle로 변환한 새로운 배열 y가 생성된다. (이때 자료형은 bool)

우리가 원하는 계단함수는 0이나 1의 int형을 출력하는 함수이므로, 

astype(np.int)를 통해 정수형 0과 1로

바꾸어 주는것이다.


## 3.2.3 계단 함수의 그래프


함수를 그래프로 그리기 위해선 matplotlib 라이브러리를 사용한다.



![](/assets/image/2022-02-16-19-48-50.png)

``` python
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x>0, dtype=np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # y축 범위 지정
plt.show()
```

np.arange(-5.0, 5.0, -0.1) 은 -5.0에서 5.0 전까지 0.1 간격의 

넘파이 배열을 생선한다.

즉, [-5.0, -4.9, -4.8, ... , 4.9]를 생성한다.

step_function()은 인수로 받은 넘파이 배열의 원소 각각을 

인수로 계단 함수를 실행해 그 결과를 다시 배열로 만들어 돌려준다.

앞서 계단함수를 두줄로 구현했었는데, 

np.array(조건, dtype=자료형) 과 같은 방법으로 한줄로 나타낼 수 있다.


plt.plot(x, y)는 x축과 y축을 나타낸다는 의미이다.

plt.ylim(-0.1, 1.1)는 y축의 범위를 -0.1부터 1.1보다 

작을때 까지 나타낸다는 의미이다.

plt.show()는 그래프를 출력하는 함수이다.


## 3.2.4 시그모이드 함수 구현하기

```python
def sigmoid(x):
    return 1/(1+np.exp(-x))

x = np.array([-1.0, 1.0, 2.0])
sigmoid(x)
```

식은 아래 식을 그대로 return하는 1줄짜리 코드이다.



![](/assets/image/2022-02-16-19-50-52.png)

그냥 exp(-x)가 아니라 np.exp(x)를 쓴 이유는 반환값을 넘파이 배열로 받기 위해서이다.

넘파이의 브로드캐스트를 통해 넘파이 배열과 스칼라 값의 연산을

넘파이 배열의 원소 각각과 스칼라값의 연산으로 바꿔 수행가능하다.


앞서 계단함수를 그린것과 같은 방법으로 시그모이드 함수도 그래프로 그려보자.

방법은 똑같으니 안보고 그려보는것을 권장한다.

```python
x = np.arange(-5.0, 5.0, 0.1)
y=sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![](/assets/image/2022-02-16-19-51-45.png)

## 3.2.5 시그모이드 함수와 계단 함수 비교



![](/assets/image/2022-02-16-19-52-05.png)


시각적으로 가장 먼저 보이는 차이는 매끄러움의 차이이다.

시그모이드는 부드러운 곡선으로 입력에 따라 출력이 연속적으로 변화한다.

계단 함수는 0을 경계로 출력이 갑자기 바뀌어버린다.

시그모이드 함수의 이 매끈함이 신경망 학습에서 아주 중요한 역할을 한다.


두번째 차이점은 계단함수는 0과 1중 하나의 값만 리턴하는 반면

시그모이드 함수는 그 사이의 소수(0.338) 등도 리터만한다.


공통점은 입력이 작으면 출력이 0에가깝고, 입력이 크면 출력이 1에 가깝다는 것이다.

또한 입력이 아무리 작거나 커도 출력은 0과 1사이이다.


## 3.2.6 비선형 함수


계단 함수와 시그모이드 함수는 둘다 비선형 함수이다.

시그모이드 함수는 곡선, 계단 함수는 계단처럼 구부러진 직선이며 비선형 함수로 분류된다.


신경망에서는 활성화 함수로 무조건 비선형 함수를 사용해야한다.

선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문이다.


선형 함수의 문제는 층을 아무리 깊게 해도 은닉층이 없는 네트워크로도 똑같은 기능을 할 수 있다.

h(x) = cx를 활성화 함수로 사용한 3층네트워크를 생각해보자.

y(x) = h(h(h(x)))가 되고 이 계산은 y(x) = c * c * c * x처럼 

곱셈을 세번하지만, 사실 y(x) = ax와 똑같은 식이다.

a = c^3이라고 나타내면 되기 때문이다.


즉 은닉층이 없는 네트워크로 표현가능하다. 

선형 함수를 이용해서는 여러층으로 구성하는 이점을 살릴 수 없기때문에

층을 쌓는 혜택을 얻으려면 활성화 함수로 반드시 비선형 함수를 사용해야 한다.


## 3.2.7 ReLU 함수


최근에는 ReLU(Rectified Linear Unit)함수를 주로 이용한다.


ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하면 0을 출력하는 함수이다.

![](/assets/image/2022-02-16-19-52-40.png)

h(x) = { x (x>0)

{ 0 (x>=0)




``` python
def relu(x):
    return np.maximim(0, x)
```


# 3.3 다차원 배열의 계산


넘파이의 다차원 배열을 숙달하면 신경망을 구현하기 수월하다.



## 3.3.1 다차원 배열


다차원 배열은 숫자의 집합으로 N차원으로 나열하는 것을 다차원 배열이라고 한다.


배열의 차원 수는 np.ndim() 함수로 확인 가능하다.

배열의 형상은 인스턴스 변수인 shape으로 알 수 있다. => (4, )라는 것은 1차원 배열이고 4원소 4개로 구성되어 있다는 뜻이다.

A.shape는 튜플을 반환하는것에 주의하자

```pyhton
B = np.array([[1,2], [3,4], [5,6]])
print(B)
np.ndim(B)
B.shape
```
2차원 배열을 살펴보자. 3x2 배열인 B를 작성하였다.

이는 처음 차원에 원소가 3개, 다음차원에 원소가 2개 있다는 뜻이다.

만약 1 x 2 x 3 x 4 배열이라면 0차원에 1개, 

2차원은 2개, 3차원은 3개, 4 차원은 4개의 원소가 있다는 뜻이다.


2차원 배열은 matrix라고도 부르며 가로방향을 행(row), 

세로 방향을 열(column)이라고 부른다.

![](/assets/image/2022-02-16-19-54-00.png)


# 3.3.2 행렬의 곱




대표사진 삭제
사진 설명을 입력하세요.

행렬의 곱은 위와 같이 계산한다. 이를 코드로 구현하면 아래와 같다.

![](/assets/image/2022-02-16-19-54-20.png)


넘파이의 내장된 함수 np.dot() 을 이용하면 된다.

``` pyhton
A = np.array([[1,2], [3,4]])
A.shape
B = np.array([[5,6], [7,8]])
B.shape
np.dot(A,B)
```

행렬의 곱은 (N x M) * (M * N)의 형태일때만 가능하다.




``` python
A = np.array([[1,2,3], [4,5,6]])
A.shape
B = np.array([[1,2],[3,4],[5,6]])
B.shape
np.dot(A,B)
``` 


## 3.3.3 신경망에서의 행렬 곱


![](/assets/image/2022-02-16-19-55-19.png)


``` pyhton
X = np.array([1,2])
X.shape
W = np.array([[1,3,5], [2,4,6]])
print(W)
W.shape
Y = np.dot(X, W)
print(Y)
```

행렬의 곱은 신경망을 구현할 때 매우 중요하다.


# 3.4 3층 신경망 구현하기


3층 신경망 구현을 위해서는 넘파이의 다차원 배열을 사용한다.


## 3.4.1 표기법

![](/assets/image/2022-02-16-19-55-51.png)

W12 를 보면 앞서나오는 숫자는 다음층의 몇번째 뉴런인지, 뒤에 나오는 숫자는 앞 층의 몇번째 뉴런인지를 의미한다.


## 3.4.2 각 층의 신호 전달 구현하기

