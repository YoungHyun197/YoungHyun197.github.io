---
title : "Pytorch 신경망 생성"
category :
    - pytorch
tag :
    - pytorch
    - deep-learning
    - machine-learning
toc : true
toc_sticky: true
comments: true
---
신경망 생성, 선형회구, 모델 생성, 손실함수, 옵티마이저, 시각화

# 신경망 생성 
- `torch.nn` 패키지 사용
- `nn.Moudle`을 상속받고, 해당 모듈은 계층과 `output`을 반환하는 `forward` 메소드를 포함
- 파이토치에서 신경망 생성을 위한 기본 틀 

```python
1. class Net(nn.Module): ## 신경망 이름 지정후 nn.Module을 상속
 
    def __init__(self): # 상속받은 내용에 대해 초기화
        super(Net, self).__init__()

2. class MyModel(nn.Module):
    
    def __init__(self):
        super(MyModel, self).__init__()
```

- conv2d layer를 쌓을 때, 필터의 갯수를 계산하고 `in_channels`에 넣어줘야함

# 신경망 정의

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
```


```python
class Net(nn.Module):
    def __init__(self):
        # __init__ 함수에서는 신경망에 쓰일 각각의 layer들을 초기화 
        super(Net, self).__init__() 
    
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6x6 이미지 차원, 120 output
        self.fc2 = nn.Linear(120, 84) 
        self.fc3 = nn.Linear(84, 10) # 최종 output은 10으로 줄어듦

    def forward(self, x):
        # 앞서 정의해둔 layer들을 어떤 식으로 사용할지 forward에서 결정
        # 입력으로 들어온 x가 conv1을 통과후 relu를 거친후 맥스풀링 
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x)) # flat하게 펼쳐준다
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
    
    def num_falt_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s 
        
        return num_features
```

```python
net = Net()
print(net)
```

# 선형회귀 (Linear Regression) 모델 생성

## modeulse import
```python
import torch
import torch.nn as nn
import torch.optim as optim

import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
```

## 데이터 생성 및 회귀식 설정
```python
X = torch.randn(100, 1)*10
y = X + 3 * torch.randn(100, 1)
plt.plot(X.numpy(), y.numpy(), 'o')
plt.ylabel('y')
plt.xlabel('x')
plt.grid()
plt.show()  
```

![](/assets/image/2022-03-02-21-20-19.png)

## 신경망 구성
```python
# 신경망 이름을 설정해준뒤 nn.Module을 상속받는다. 
class LinearRegressionModel(nn.Module):
  # 신경망에서 사용할 layer를 정의 
  def __init__(self):
    super(LinearRegressionModel, self).__init__()
    self.linear = nn.Linear(1, 1) # 단순히 1x1입력을 받는 선형 layer

  def forward(self, x):
    pred = self.inear(x) # 입력값을 받은후 선형 layer를 통과 시켜준 값을 반환
    return pred

  # 단순 하나의 layer만 존재하는 신경망 

```
```python
torch.manual_seed(111)

model = LinearRegressionModel()
print(model)

print(list(model.parameters()))
```
```python
w, b = model.parameters()
def get_params():
  return w[0][0].item(), b[0].item()

def plot_fit(title):
  plt.title = title
  w1, b1 = get_params()
  x1 = np.array([-30, 30])
  y1 = w1 * x1 + b1
  plt.plot(x1, y1, 'r')
  plt.scatter(X, y)
  plt.show()

plot_fit('Initial Model')

```
![](/assets/image/2022-03-02-21-20-40.png)

붉은선은 학습전 초기 모델의 가중치와 bias를 의미한다.

우리는 이 붉은선을 현재 데이터에 맞게 업데이트 해주는 것이 목표이다.

# 손실함수와 옵티마이저
```python
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)
```

# 모델 학습
```python
epochs = 100 # 100번 반복
losses = [] 
for epoch in range(epochs):
  optimizer.zero_grad() # 매개변수 0으로 초기화  

  y_pred = model(X) # 모델의 예측값 
  loss = criterion(y_pred, y) # MSE loss 사용
  losses.append(loss) # 리스트에 loss값들을 추가
  loss.backward() # 역전파

  optimizer.step() # lr을 반영

  if epoch % 10 ==0:
    print(f"Epoch: {epoch+1}\t {loss:.4f}")
```

![](/assets/image/2022-03-02-21-48-47.png)

loss가 초기에 47에서 감소하긴하지만 10.7정도 까지 밖에 떨어지지 않는다.

# 시각화
```python
plt.plot(range(epochs), torch.Tensor(losses))
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.show()
```
![](/assets/image/2022-03-02-21-54-33.png)

```python
plot_fit("Trained Model")
```
![](/assets/image/2022-03-02-21-54-52.png)

학습을 통해 우리가 원하는 모양으로 맞춰진 것을 확인 가능하다. 

