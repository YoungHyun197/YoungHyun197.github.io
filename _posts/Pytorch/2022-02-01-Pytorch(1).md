---
title : "Pytorch Tutrial(1)"
category :
    - pytorch
tag :
    - pytorch
    - deep-learning
    - machine-learning
toc : true
toc_sticky: true
comments: true
---
Pytorch의 텐서와 넘파이에 대하여 알아보자.


# 텐서(Tensor)

텐서는 배열이나 행렬과 매우 유사한 자료구조이다.
Pytorch에서 텐서를 사용하여 모델 입출력 뿐만 아니라 매개변수를 부호화한다.

GPU 등 특수한 하드웨어에서 실행가능 하다는 점을 제외하면 numpy의 ndarray와 매우 유사하다.

```python
import torch
import numpy as np
```

### 텐서 초기화

```
텐서는 여러가지 방법으로 초기화 가능
데이터로 부터 직접 텐서 생성가능
자료형은 자동으로 유추
```

```python
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
```

### 1. numpy 배열로부터 생성

텐서는 Numpy 배열을 통해 생성가능. 반대도 가능

```python
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
```

### 2. 다른 텐서로 부터 생성

override하지 않으면 속성(shape)과 자료형을 유지한다.

```python
x_ones = torch.ones_like(x_data) #x_data의 속성을 유지
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data 속성 덮어씌움
print(f"Rnadom Tensor: \n {x_rand} \n")
```

    Ones Tensor:
     tensor([[1, 1],
            [1, 1]])

    Rnadom Tensor:
     tensor([[0.0899, 0.4446],
            [0.9243, 0.5360]])

### 3. random 또는 constant 값 사용

`shape`은 텐서의 차원을 나타내는 튜플. 출력 텐서의 차원 결정

```python
shape = (2, 3,) # 2행 3열
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n{rand_tensor}\n")
print(f"Ones Tensor : \n{ones_tensor}\n")
print(f"Zeros Tnesor : \n{zeros_tensor}\n")
```

    Random Tensor:
    tensor([[0.0900, 0.5700, 0.4209],
            [0.9162, 0.9534, 0.8413]])

    Ones Tensor :
    tensor([[1., 1., 1.],
            [1., 1., 1.]])

    Zeros Tnesor :
    tensor([[0., 0., 0.],
            [0., 0., 0.]])

### 텐서의 속성(Attribute)

텐서의 속성은 shape, 자료형, 어느 장치에 저장되는지를 의미

```python
tensor = torch.rand(3, 4) # 3x4 행렬 생성

print(f"Shpae of tensor: {tensor.shape}")
print(f"Datatype of tensor : {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")
```

    Shpae of tensor: torch.Size([3, 4])
    Datatype of tensor : torch.float32
    Device tensor is stored on: cpu

### 텐서 연산

Transpose, Indexing, Slicing, 수학 계산, 선형대수, 임의 샘플링등
다양한 텐서연산가능.

각 연산들은 GPU에서 CPU보다 빠르게 실행가능. GPU가 없다면 colab 사용 권장

```python
# GPU가 존재하면 텐서를 이동
if torch.cuda.is_available():
    tensor = tensor.to('cuda')
    print(f"Device tensor is storde on: {tensor.device}")
```

### Numpy와 같은 방법의 인덱싱과 슬라이싱

```python
tensor = torch.ones(4,4) # 4x4 행렬 1로 초기화
tensor[:, 1] = 0 ## 1열을 0으로 초기화
print(tensor)
```

    tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])

### Tensor 합치기

`torch.cat` 을 통해 텐서 결합

```python
t0 = torch.cat([tensor, tensor, tensor], dim=0) # 0차원 증가 4x4->12x4
print(t0)
print(t0.shape)
t1 = torch.cat([tensor, tensor, tensor], dim=1) # 1차원 증가 4x4->4x12
print(t1)
print(t1.shape)
```

    tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])
    torch.Size([12, 4])
    tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])
    torch.Size([4, 12])

### 텐서 곱하기

**요소별 곱
`tensor.mul(tensor)`**

```python
# 요소별 곱
print(f"tensor.mul(tensor)\n {tensor.mul(tensor)} \n")
# 다른 문법
print(f"tensor * tensor \n {tensor * tensor} \n")
```

    tensor.mul(tensor)
     tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])

    tensor * tensor
     tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])

**행렬 곱 `tensor.matmul(tensor.T)`**

```python
print(f"tensor.matmul(tensor.T)\n {tensor.matmul(tensor.T)}\n")
# 다른 문법
print(f"tensor @ tensor.T \n {tensor @ tensor.T} \n")
```

    tensor.matmul(tensor.T)
     tensor([[3., 3., 3., 3.],
            [3., 3., 3., 3.],
            [3., 3., 3., 3.],
            [3., 3., 3., 3.]])

    tensor @ tensor.T
     tensor([[3., 3., 3., 3.],
            [3., 3., 3., 3.],
            [3., 3., 3., 3.],
            [3., 3., 3., 3.]])

**바꿔치기(in-place)연산**

```
`_`가 붙으면 바꿔치기 연산
`x.copy_()` 나 `x._t()`는 `x`를 변경

```

in-place연산은 메모리를 일부 절약하지만 기록이 즉시 삭제되어
도함수 계산에 문제발생 위험. 따라서 사용 권장 X

```python
print(tensor, "\n")
tensor.add_(5)
print(tensor)
```

    tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])

    tensor([[6., 5., 6., 6.],
            [6., 5., 6., 6.],
            [6., 5., 6., 6.],
            [6., 5., 6., 6.]])

# Numpy 변환(Bridge)

CPU 상의 텐서와 numpy배열은 메모리 공간을 공유하기 때문에
하나를 변경시 다른 하나도 변경된다.

### 텐서를 Numpy 배열로 변환

```python
t = torch.ones(5)
print(f"t: {t}")
n = t.numpy()
print(f"n: {n}")
```

    t: tensor([1., 1., 1., 1., 1.])
    n: [1. 1. 1. 1. 1.]

### 텐서의 변경사항이 Numpy 배열에 반영

```python
t.add_(1)
print(f"t: {t}")
print(f"n: {n}")
```

    t: tensor([2., 2., 2., 2., 2.])
    n: [2. 2. 2. 2. 2.]

### Numpy 배열을 텐서로 변환

```python
n = np.ones(5)
t = torch.from_numpy(n)
```

### Numpy 배열의 변경 사항이 텐서에 반영

```python
np.add(n, 1, out=n)
print(f"t: {t}")
print(f"n: {n}")
```

    t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
    n: [2. 2. 2. 2. 2.]
