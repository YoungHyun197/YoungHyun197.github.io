---
title : "Pytorch Tutrial(3)"
category :
    - pytorch
tag :
    - pytorch
    - deep-learning
    - machine-learning
toc : true
toc_sticky: true
comments: true
---
Pytorch의 nn.Module 에 대하여 알아보자.



# nn.Moudule
`nn.Module`은 계층(layer)과 output을 반환하는 `forward(input)`를 포함한다.

신경망의 일반적인 학습과정을 살펴보자.
1. 학습 가능한 매개변수를 갖는 신경망 정의
2. 데이터셋 입력을 반복
3. 입력을 신경망에서 전파 (forward)
4. loss를 계산 
5. gradient를 신경망의 매개변수들에 역전파 (backward)
6. 신경망의 가중치 갱신. `w = w - lr * gradient`
  

# 신경망 정의 
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

  def __init__(self):
    super(Net, self).__init__()
    # 입력 이미지 채널 1개, 출력 채널 6개, 5x5 정사각 컨볼루션 행렬
    # 컨볼루션 커널 정의
    self.conv1 = nn.Conv2d(1, 6, 5)
    self.conv2 = nn.Conv2d(6, 16, 5)
    ## affine 연산 : y = Wx + b
    self.fc1 = nn.Linear(16 * 5 * 5, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)

  def forward(self, x):
    #(2,2)크기 윈도우에 대해 맥스풀링(max pooling)
    x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))
    # 크기가 제곱수라면, 하나의 숫자만을 특정
    x = F.max_pool2d(F.relu(self.conv2(x)), 2)
    x = torch.flatten(x, 1) # 배치 차원 제외 모든 차원 평탄화
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return x

net = Net()
print(net)

```

forwrad 함수만 정의하면 `backward` 함수는 `autograd`를 사용하여 자동으로 정의된다.

`forward` 함수에서는 어떠한 Tensor 연산을 사용해도 된다.

모델의 학습 가능한 매개변수들은 `net.parmeters()`에 의해 반환된다.

```python
params = list(net.parameters())
print(len(params))
print(params[0].size()) # conv1의 .weight
```

임의의 32 x 32 입력값을 넣어보자.

```python
input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)
```

모든 매개변수의 변화도 버퍼(gradient buffer)를 0으로 설정하고, 무작위 값으로 역전파.

```python
net.zero_grad()
out.backward(torch.randn(1, 10))
```

`torch.nn`은 미니배치만 지원한다.<br/>
`torch.nn` 패지지 전체는 하나의 샘플이 아닌, 샘플들의 미니배치만을 입력으로 받는다.<br/>
만약 하나의 샘플만 있다면, `input.unsqeeze(0)`을 사용해서 가상의 차원을 추가한다.<br/>

## 요약:
- `torch.Tensor` - `backward()`와 같은 autograd 연산을 지원하는 다차원 배열
  > tensor에 대한 변화도를 갖고 있음
- `nn.Moudule` - 신경망 모듈.
  > 매개변수를 캡슐화하는 간편한 방법 <br/>
  GPU이동, 내보내기, 불러오기 등 작업을 위한 helper 제공

- `nn.Parameter` - Tensor의 한 종류로, `Module` 에 속성으로 할당될 때 자동으로 매개변수로 등록
- `atuograd.Function` - `autograd` 연산의 순방향과 역방향 정의를 구현
  > 모든 `Tensor` 연산은 하나 이상의 `Function` 노드를 생성<br/>
  각 노드는 `Tensor`를 생성하고 이력(history)를 인코딩하는 함수들과 연결

신경망을 정의하는 방법과 입력을 처리하고 `backward`를 호출하는 방법을 알아보았다.<br/>

- 손실을 계산하는 방법
- 신경망의 가중치를 갱신하는 방법 <br/>
에 대해 알아보자.

# 손실 함수 (Loss Function)
손실 함수는 (output, target)을 한 쌍의 입력으로 받아 <br/>
output이 target으로 부터 얼마나 멀리 떨어져 있는지 추정하는 값을 계산한다. <br/>

nn 패키지에는 여러가지의 `손실 함수들`이 존재한다.<br/>
간단한 손실함수로는 출력과 대상간의 `평균제곱오차`를 계산하는 `nn.MSEloss`가 있다.<br/>

```python
output = net(input)
target = torch.randn(10) 
target = target.view(1,-1) # 출력과 같은 shape으로 만듦

loss = crieterion(output, target)
print(loss)
```

`.grad.fn` 속성을 사용하여 `loss`를 역방향으로 따라가다보면<br/>
다음과 같은 연산 그래프를 볼 수 있다.

```python
input -> conv2d -> relu -> conv2d -> relu -> maxpool2d 
-> flatten -> linear -> relu -> linear -> relu -> linear
-> MSELoss
-> loss
```
따라서 `loss.backward()`를 실행 시, 전체 그래프는 신경망의 <br/>
매개변수에 대해 미분되며, `requires_grad=True`인 모든<br/>
Tensor는 변화도가 누적된 `.grad` Tensor를 갖게된다.

```python
print(loss.grad_fn) # MSELoss
print(loss.frad_fn.next_function[0][0]) # Linear
print(loss.grad_fn.next_function[0][0].next_function[0][0]) # ReLU
```

# 역전파 (Backprop)
error를 역전파하기 위해서는 `loss.backward()`만 해주면 된다.

기존에 계산된 변화도의 값을 누적시키고 싶지 않다면 <br/>
기존에 계산된 변화도를 0으로 만드는 작업이 필요

```python
net.zero_grad() # 모든 배개변수의 변화도 버퍼를 0으로 만듦

print(`conv1.bias.grad before backward`)
print(net.conv1.bias.grad)

loss.backward()

print(`conv1.bias.grad after backward`)
print(net.conv1.bias.grad)
```

# 가중치 갱신
많이 사용되는 가장 단순한 갱신법은 확률적 경사하강법이다.

`새로운 가중치(w)` = `가중치(w)` - `학습률(lr)` * `변화도(grad)`

```python
lr = 0.01
for f in net.parmeters():
  f.data.sub_(f.grad.data * lr)
```

신경망 구성시 SGD, Nestrov-SGD, Adam, RMSProp등 다양한 갱신규칙을 사용하고 싶다면<br/>
`torch.optim` 패키지를 이용하면 된다.

```python
import torch.opitm as optim

# Optimizer를 생성
optimizer = optim.SGD(net.parameters(), lr = 0.01)

# 학습과정
optimizer.zero_grad() # 변화도 버퍼를 0으로
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step() # 업데이트 진행*
```
`optimizer.zero_grad()` 를 사용하여 수동으로 변화도 <br/>
버퍼를 0으로 설정하는 것에 유의<br/>
`역전파(backprop)`에서 설명한것 처럼 변화도가 누적되기 때문<br/>




