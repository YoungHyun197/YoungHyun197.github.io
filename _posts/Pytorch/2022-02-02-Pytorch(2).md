---
title : "Pytorch Tutrial(2)"
category :
    - CS231n
tag :
    - pytorch
    - deep-learning
    - machine-learning
toc : true
toc_sticky: true
comments: true
---
Pytorch의 autograd에 대하여 알아보자.


# torch.autograd

`torch.atutograd` 는 신경망 학습을 지원하는 PyTorch의 자동 미분 엔진

### 배경(Background)

신경망은 어떤 입력 데이터에 대해 실행되는 중첩된 함수들의 모음
이 함수들은 PyTorch에서 Tensor로 저장되는 가중치(weight)와 편향(bias)로 구성된 매개변수들로 정의 된다.

신경망 학습은 2단계로 구성

1. 순전파(Forward Propagation)

- 신경망은 정답을 맞추기 위해 최선의 추측을 한다.
- 추측을 위해 입력데이터를 각 함수들에서 실행함

  2.역전파(Backward Propagation)

- 신경망은 추측한 값에서 발생한 오류(error)에 비례하여(proportionate) 매개변수들을 적절히 조절(adjust)
- 출력(output)로 부터 역방향으로 이동하면서 오류에 대한 함수들의 매개변수들의 미분값을 수집
- 경사하강법을 사용하여 매개변수들을 최적화

```python
%matplotlib inline # 브라우저에서 바로 그림을 볼 수 있게해줌
```

### Pytorch에서 사용법

`torchvision`에서 미리 학습된 resnet18 모델을 불러온다.<br/>
3채널 짜리 높이와 넓이가 64인 이미지 하나를 표현하는 랜덤 텐서를 생성 <br/>
label(정답)을 무작위 값으로 초기화

```python
import torch, torchvision
model = torchvision.models.resnet18(pretrained=True)
data = torch.rand(1, 3, 64, 64)
labels = torch.rand(1, 1000)
```

**1단계** **순전파** <br/>
입력(input) 데이터를 모델의 각 층(layer)에 통과시켜 예측값을 생성

```python
prediction = model(data) # 순전파 단계 (forward pass)
```

**2단계 오차구하기** 모델 예측값과 그에 해당하는 정답(lable)을 오차(error, `손실(loss)`)를 계산<br/>
**3단계 역전파** <br/>
오차 텐서(error tensor)에 `.backward()`를 호출하면 역전파가 시작
Autograd가 매개변수(parameter)의 `.grad` 속성(attribute)에
모델의 각 매개변수에 대한 변화도(gradient)를 계산하고 저장

```python
loss = (prediction - labels).sum()
loss.backward() # 역전파 단계(backward pass)
```

**4단계 최적화** : 옵티마이저(optimizer) 불러오기 <br/>

- 학습율(learning rate) 0.01과 모멘텀(momentum) 0.9를 갖는 SGD
- optimizer에 모델의 모든 매개변수를 등록

```python
optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
```

**5단계 경사하강법** `.step`을 호출하여 경사하강법 시작 <br/>
optimizer는 `.grad`에 저장된 변화도에 따라 각 매개변수를 조정

```python
optim.step() # 경사하강법(gradient descent)
```

# autograd 동작 세부설명

`autograd`가 어떻게 변화도(gradient)를 수집할까? <br/>
`requires_grad=True` 를 갖는 2개의 텐서(tesnor) `a`와 `b`를 생성 <br/>
`requires_grad=True` 는 `autograd`에 모든 연산(operation)들을 추적해야 한다고 알려준다.

```python
a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)
```

`a`와 `b`로 부터 새로운 텐서 `Q`를 생성 <br/>
\begin{align}Q = 3a^3 - b^2\end{align}

```python
Q = 3*a**3 - b**2
```

`a` 와 `b`는 신경망의 매개변수, `Q`는 오차라고 가정 <br/>
신경망 학습을 위해 우리는 매개변수들에 대한 오차의 변화도(gradinet)를 구해야 한다<br/>
즉,<br/>

<p align="center">

$${
\frac{\partial Q}{\partial a} = 9a^2 
}$$

$${
\frac{\partial Q}{\partial b} = -2b
}$$

</p>

 <br/>
`Q`에 대해서 `.backward()`를 호출 시, autograd는 변화도들을 계산<br/>
이를 각 텐서의 `.grad` 속성에 저장<br/><br/>

`Q`는 벡터이므로 `Q.backward()`에 `gradient` 인자를 명시적으로 전달해야함<br/>
`gradient`는 `Q`와 같은 모양의 텐서로 `Q`자기자신에 대한 gradeint를 의미<br/>

즉, <br/>
<p align="center">
$$
{\frac{dQ}{dQ} = 1}
$$
<br/>
</p>
동일하게, `Q.sum().backward()`와 같이 `Q` 스칼라 값으로 집계한뒤 <br/>
암시적으로 `.backward()`를 호출할 수도 있다.

```python
external_grad = torch.tensor([1., 1.])
Q.backward(gradient = external_grad)
```

이제 변화도는 a.grad와 b.grad에 저장

```python
# 수집된 변화도가 올바른지 확인
print(9*a**2==a.grad)
print(-2*b == b.grad)
```

    tensor([True, True])
    tensor([True, True])

# 부록 - `autograd`를 사용한 벡터 미적분

수학적으로, <br/>
벡터 함수 $\vec{y}=f(\vec{x})$ 에서 $\vec{x}$ 에
대한 $\vec{y}$ 의 변화도는 야코비안 행렬(Jacobian Matrix) $J$ 이다.
<p align="center">
$${
J
     =
      \left(\begin{array}{cc}
      \frac{\partial \bf{y}}{\partial x_{1}} &
      ... &
      \frac{\partial \bf{y}}{\partial x_{n}}
      \end{array}\right)
     =
     \left(\begin{array}{ccc}
      \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\
      \vdots & \ddots & \vdots\\
      \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
      \end{array}\right)
}$$
</p>

<br/>
일반적으로, `torch.autograd` 는 벡터-야코비안 곱을 계산하는 엔진.<br/>
이는, 주어진 어떤 벡터 $\vec{v}$ 에 대해 $J^{T}\cdot \vec{v}$ 을 연산<br/>

만약 ${\vec{v}}$ 가 스칼라 함수 ${l=g\left(\vec{y}\right)}$ 의 변화도(gradient)인 경우:

<p align="center">
$${
\begin{align}\vec{v}
   =
   \left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} & \cdots & \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}\end{align}
}$$
</p>

이며, 연쇄 법칙(chain rule)에 따라, 벡터-야코비안 곱은 ${\vec{x}}$ 에 대한
${l}$ 의 변화도(gradient)가 된다.

<p align="center">
$${ 
J^{T}\cdot \vec{v}=\left(\begin{array}{ccc}
      \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{1}}\\
      \vdots & \ddots & \vdots\\
      \frac{\partial y_{1}}{\partial x_{n}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
      \end{array}\right)\left(\begin{array}{c}
      \frac{\partial l}{\partial y_{1}}\\
      \vdots\\
      \frac{\partial l}{\partial y_{m}}
      \end{array}\right)=\left(\begin{array}{c}
      \frac{\partial l}{\partial x_{1}}\\
      \vdots\\
      \frac{\partial l}{\partial x_{n}}
      \end{array}\right)
      }$$
</p>
이러한 특성을 사용했다.
`external_grad` 가 ${\vec{v}}$ 를 뜻한다.

### 연산 그래프(Computational Graph)

개념적으로, autograd는 데이터(텐서)의 및 실행된 모든 연산들(및 연산 결과가 새로운 텐서인 경우도 포함하여)의
기록을 `Function` 객체로
구성된 방향성 비순환 그래프(DAG; Directed Acyclic Graph)에 저장(keep)한다.
이 방향성 비순환 그래프(DAG)의 잎(leave)은 입력 텐서이고, 뿌리(root)는 결과 텐서이다.
이 그래프를 뿌리에서부터 잎까지 추적하면 연쇄 법칙(chain rule)에 따라 변화도를 자동으로 계산할 수 있다.

순전파 단계에서, autograd는 다음 두 가지 작업을 동시에 수행:

- 요청된 연산을 수행하여 결과 텐서를 계산하고,
- DAG에 연산의 _변화도 기능(gradient function)_ 를 유지(maintain).

역전파 단계는 DAG 뿌리(root)에서 `.backward()` 가 호출될 때 시작. `autograd` 는 이 때:

- 각 `.grad_fn` 으로부터 변화도를 계산하고,
- 각 텐서의 `.grad` 속성에 계산 결과를 쌓고(accumulate),
- 연쇄 법칙을 사용하여, 모든 잎(leaf) 텐서들까지 전파(propagate).

다음은 지금까지 살펴본 예제의 DAG를 시각적으로 표현한 것이다. 그래프에서 화살표는 순전파 단계의 방향을 나타낸다.
노드(node)들은 순전파 단계에서의 각 연산들에 대한 역전파 함수들을 나타낸다. 파란색 잎(leaf) 노드는 잎 텐서 `a` 와 `b` 를 나타낸다.

<div class="alert alert-info"><h4>Note</h4><p>
PyTorch에서 DAG들은 동적(dynamic)이다.
  주목해야 할 중요한 점은 그래프가 처음부터(from scratch) 다시 생성된다는 것; 매번 .backward() 가
  호출되고 나면, autograd는 새로운 그래프를 채우기(populate) 시작한다. 이러한 점 덕분에 모델에서
  흐름 제어(control flow) 구문들을 사용할 수 있게 되는 것; 매번 반복(iteration)할 때마다 필요하면
  모양(shape)이나 크기(size), 연산(operation)을 바꿀 수 있다.</p></div>

### DAG에서 제외하기

`torch.autograd` 는 `requires_grad` 플래그(flag)가 `True` 로 설정된 모든 텐서에 대한
연산들을 추적. 따라서 변화도가 필요하지 않은 텐서들에 대해서는 이 속성을 `False` 로 설정하여
DAG 변화도 계산에서 제외한다.

입력 텐서 중 단 하나라도 `requres_grad=True` 를 갖는 경우, 연산의 결과 텐서도 변화도를 갖게 된다.

```python
x = torch.rand(5, 5)
y = torch.rand(5, 5)
z = torch.rand((5, 5), requires_grad=True)

a = x + y
print(f"Does `a` require gradients? : {a.requires_grad}")
b = x + z
print(f"Does `b` require gradients?: {b.requires_grad}")
```

    Does `a` require gradients? : False
    Does `b` require gradients?: True

신경망에서, 변화도를 계산하지 않는 매개변수를 일반적으로 고정된 매개변수(frozen parameter) 이라고 부른다. 이러한 매개변수의 변화도가 필요하지 않다는 것을 미리 알고 있으면, 신경망 모델의 일부를 “고정(freeze)”하는 것이 유용하다. (이렇게 하면 autograd 연산량을 줄임으로써 성능 상의 이득을 제공)

DAG에서 제외하는 것이 중요한 또 다른 일반적인 사례(usecase)는 미리 학습된 모델을 미세조정 하는 경우이다.

미세조정(finetuning)을 하는 과정에서, 새로운 정답(label)을 예측할 수 있도록 모델의 대부분을 고정한 뒤 일반적으로 분류 계층(classifier layer)만 변경. 이를 설명하기 위해 간단한 예제를 살펴보자. 이전과 마찬가지로 이미 학습된 resnet18 모델을 불러온 뒤 모든 매개변수를 고정한다.

```python
from torch import nn, optim

model = torchvision.models.resnet18(pretrained=True)

# 신경망의 모든 매개변수를 고정
for param in model.parameters():
    param.requires_grad = False
```

10개의 정답(label)을 갖는 새로운 데이터셋으로 모델을 미세조정하는 상황을 가정. resnet에서 분류기(classifier)는 마지막 선형 계층(linear layer)인 model.fc 이다. 이를 새로운 분류기로 동작할 (고정되지 않은) 새로운 선형 계층으로 간단히 대체.

```python
model.fc = nn.Linear(512, 10)
```

이제 model.fc 를 제외한 모델의 모든 매개변수들이 고정되었다.<br/>
변화도를 계산하는 유일한 매개변수는 model.fc 의 가중치(weight)와 편향(bias)뿐.

```python
# 분류기만 최적화.
optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
```

옵티마이저(optimizer)에 모든 매개변수를 등록하더라도,<br/>
변화도를 계산(하고 경사하강법으로 갱신)할 수 있는 매개변수들은<br/>
분류기의 가중치와 편향뿐이다.<br/>

컨텍스트 매니저(context manager)에 torch.no_grad() 로도 <br/>
똑같이 제외하는 기능을 사용할 수 있다.

