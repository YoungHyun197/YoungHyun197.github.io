---
title : "CS231n 4강 Neural Network"
category :
    - CS231n
tag :
    - machine_learning
    - CS231n
    - computer vision
toc : true
toc_sticky: true
comments: true
---
신경망에 대하여 알아보자
# Computational Graph
지난 시간에 우리는 `Gradient Descent`에 대해 배웠다. <br/>
`Gradient Descent`란 가장 loss가 낮은 곳을 찾기위한 방법이고 `optimization`을 위해<br/> 
사용되는 기법이다.

출력값을 알고있을 때, 그때의 gradient를 이용하면 입력값의 gradient도 계산이 가능하다.<br/>
즉 결과로부터 입력의 gradient를 계산하므로 `Backpropagation(역전파)` 라고 부른다.

이해를 돕기위해 그래프를 이용하는데 이것을 `Computational Graph`라고 한다.

수학적인 내용이 나오나, 크게 어렵지 않으므로 천천히 완벽하게 이해해볼 것을 추천한다.

`Computational Graph`를 이해하는 것은 앞으로 딥러닝관련 공부함에 있어 아주 기초가 되기 때문에 

반드시 정확하게 이해하고 넘어가자.

$$ 
f(x, y, z) = (x+y)z
$$

다음과 같은 식이 있을때,<br/>
x = -2, y = 5, z= -4 라고 하자.<br/>

해당 연산을 그래프로 표현하면 다음과 같다. 
![](/assets/image/2022-02-09-15-18-03.png)

x 와 y를  덧셈노드로 묶어준뒤에 해당 결과를 z와 곱셈노드로 묶어준 것이 f 이다.

역전파 역시 해당 계산그래프에 표현이 가능하다. <br/>

역전파를 위해 우리는 `Chain Rule`을 정확히 이해할 필요가있다.

`Chain Rule`을 적용하기 위해 우리는 변수 `q`를 도입한다.
이때, 
$$ q = x+y \qquad \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1
$$

`f` 를 `q`를 이용해 다시 표현하면 
$$
f= qz \qquad \frac{\partial f}{\partial q} = z , \frac{\partial f}{\partial z} = q
$$

우리가 알고 싶은 내용은 입력에 대한 gradient이다. 즉,
$$ \frac{\partial f}{\partial x} , \frac{\partial f}{\partial y} , \frac{\partial f}{\partial z} 
$$

결과 f를 알때, 입력에 대한 gradient를 구하는 과정 (Backpropagation)을 <br/>
천천히 계산 그래프에 나타내보자.

![](/assets/image/2022-02-09-15-30-10.png)

우선 ${ \frac{\partial f}{ \partial z}}$ = x + y 이므로 -2 + 5 = 3 임을 쉽게 구할수 있다.<br/>


![](/assets/image/2022-02-09-15-34-23.png) <br/>
그 다음은 ${ \frac{\partial f}{\partial q}= z}$ 이므로 -4 가 된다.  

이제 순서대로 ${\frac{\partial f}{ \partial y}}$ , ${\frac{\partial f}{ \partial x}}$ 를 구해보면  <br/>

$$\frac{\partial f}{ \partial y} = \frac{\partial f}{ \partial q} \frac{\partial q}{ \partial y} = z * 1 = -4 $$
$$\frac{\partial f}{ \partial x} = \frac{\partial f}{ \partial q} \frac{\partial q}{ \partial x} = z * 1 = -4 $$

임을 알 수 있다.  <br/>

우리는 이것을 단순히 `chain rule`을 적용해 구한다기 보다는 <br/>
관점의 변화를 통해 보다 쉽게 이해할 수 있다. <br/>

![](/assets/image/2022-02-09-15-39-27.png)

신경망 전체에서 일부를 따온 그림이다. <br/>

해당 부분만 지켜보았을때 우리는 입력 `x`와 `y`를 받아 `f`라는 함수를 거친다. <br/>

`f` 함수를 거치면 `z` 라는 출력값이 나온다. 

`f`를 기준으로 `local gradient`를 구하면 출력값은 `z`이고 각 입력에 대해 (여기서는 x,y) `gradient`를

구하는 것으로 ${ \frac{\partial z}{\partial x} }$, ${ \frac{\partial L}{\partial y} }$ 가 f의 `local gradeint`가 된다. 

그림의 맨 뒤에서 부터 `backpropagation` 과정을 살펴보면 `z`를 기준으로 `gradient`를 구하게 된다.

`L(Local gradient의 약자)`은 상류로부터 흘러온 gradient값이라고 이해하면 된다.

`z`를 기준으로 `gradient`를 구하면 ${ \frac{\partial L}{\partial z} }$ 가 되고

이 값이 역전파를 통해 하류로 흐르게 된다. 

따라서 `x`와 `y`를 기준으로 `gradient`를 구하게 되면 

$$ \frac{\partial L}{\partial x}  =  \frac{\partial L}{\partial z}\frac{\partial z}{\partial x}$$
$$ \frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial x} $$

가 된다. 자세히 살펴보면 `local gradient`에 상류로부터 흘러들어온 `grdient`를 곱해준 값이다. <br/>

따라서 계산그래프를 통해 gradient를 구하는 과정에서 `gradient` 값은 해당 노드를 기준으로 구한 

`local gradient` 에 `upstream gradient`를 곱해주는 과정을 통해 손쉽게 구할 수 있다. 


지금까지의 과정이 잘 이해되지 않는 다면 잠시 멈추고 손으로 직접 계산해보고

확실이 이해하고 넘어가는 것이 중요하다. 

## 연습문제 
$$ f (w , x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2x_2)}} $$

잠시 멈추고 해당 식에대해 직접 `computational graph`를 그리고 각각의 `gradient`를 구해보자.

앞선 내용을 이해했다면 아주 손쉽게 정답을 구할 수 있다. 

우선 계산그래프를 그리면 다음과 같다. 

![](/assets/image/2022-02-09-16-01-44.png)


맨 뒤에서 부터 차례대로 gradient 값을 계산해 나가보자.

 - ${\frac{\partial f}{\partial f} = 1.00 }$
 - 1/x 노드는 ${q(x) = \frac{1}{x}}$ 
   > 이때, local gradient는 ${\frac{-1}{x^2}}$, 입력값 = 1.37 이므로 ${\frac{-1}{1.37^2} = -0.53}$<br/>
    `local gradient` * `upstream gradient` = -0.53 * 1 = -0.53
 - +1 노드는 q(x) = x + 1 
   > 이때, local gradient는 1이된다. <br/>
   `local gradient` * `upstream gradient` = 1 * -0.53 = -0.53
 - exp 노드는 ${q(x) = e^x}$ 
   > 이때, local gradient = ${e^x}$ , 입력값 -1.00 이므로 ${e^{-1} }$<br/>
    `local gradient` * `upstream gradient` = ${e^-1}$ * -0.53 = -0.20
 - *-1 노드는 $q(x) = -x$
   > 이때, local gradient = -1 <br/>
    `local gradient` * `upstream gradient` = -1 * -0.20 = 0.20
 -  `+` 노드는 q(x,y) = x + y ${\frac{\partial q}{\partial x} = 1.00}$, ${\frac{\partial q}{\partial y} = 1.00 }$ <br/>
    > ${\frac{\partial f}{\partial w_2} =}$ `local gradient` * `upstream gradient` = 1 * 0.20 = 0.20 <br/>
    > 위로 역전파시 `local gradient` * `upstream gradient` = 1.00 * 0.20 = 0.20
 -  이후 나오는 마찬가지로`+` 노드는 q(x,y) = x + y ${\frac{\partial q}{\partial x} = 1.00}$, ${\frac{\partial q}{\partial y} = 1.00 }$ <br/>
    > 즉, 덧셈노드는 값을 그대로 전파함을 알 수 있고, 따라서 각  bracnh로 0.20 전파
- 아래 `*` 노드는 q(x, y) = x * y, ${\frac{\partial q}{\partial x} = y}$, ${\frac{\partial q}{\partial y} = x }$  <br/>
   > 따라서 아래로 역전파시  <br/>
   ${\frac{\partial f}{\partial x_1} =}$ `local gradient` * `upstream gradient` = -3.00 * 0.20 = -0.60 <br/>
   > 위로 역전파시 <br/>
    ${\frac{\partial f}{\partial w_1} =}$  `local gradient` * `upstream gradient` = -2.00 * 0.20 = -0.40 <br/>

- 위 `*` 노드는 q(x, y) = x * y, ${\frac{\partial q}{\partial x} = y}$, ${\frac{\partial q}{\partial y} = x }$  <br/>
   > 따라서 아래로 역전파시  <br/>
   ${\frac{\partial f}{\partial x_0} =}$ `local gradient` * `upstream gradient` = 2.00 * 0.20 = 0.40 <br/>
   > 위로 역전파시 <br/>
    ${\frac{\partial f}{\partial w_0} =}$  `local gradient` * `upstream gradient` = -1.00 * 0.20 = -0.20 <br/>
 

 
 우리는 이 과정을 통해 어느정도 직관을 기를수 있다. <br/>
 -  `add` gate : `upstream`의 값을 그대로 흘려준다.
 -  `mul` gate : `upsteram`의 값에 상대방 branch 값을 곱해준다. 
  
위 식에는 나오지 않았지만 `max` gate는 max에겐 값을 통과시키고 아닌 값엔 0을 흘려준다.<br/>

강의에서는 각각 3개의 gate를 다음과 같이 표현한다. 
![](/assets/image/2022-02-09-17-13-22.png)

## 정리
지금까지의 내용을 정리하면 우리는 인접한 노드들과 결과노드의 gradient값을 알면<br/>
`local gradient` * `upstream gradient` 를 통해 손쉽게 입력값에 대한 미분값을 구할 수 있다.<br/>


위 과정을 확장하면 Vector에 대해서도 적용가능하다. 

![](/assets/image/2022-02-09-17-18-43.png)

조금 어려울 수 있지만 강의내용을 따라 하나씩 적어보면 이해할 수 있을 것이다.

지금까지 배운 `Computational Graph`를 `forward` 와 `backward`로 구성된 Module로 표현가능하다. 이렇게하면 쉽게 각각의 API를 사용가능하다. 

```python

class ComputationalGraph(object):
   #...
   def forward(inputs): # forward는 input값에 함수를 적용해 위로 흘려보내는것.
      # 1. [pass inputs to input gates...]
      # 2. forward the computational graph:
      for gate in self.graph.nodes_topologically_sorted():
         gate.forward()
      return loss # the final gate in the graph outputs the loss
   def backward():
      for gate in reversed(self.graph.nodes_topologically_sorted()):
         gate.backward() # little piece of backprop (chain rule applied) 
      return inputs_gradients
```

위 logic을 적용한 `*` gate는 아래와 같이 표현가능하다. 
```python 
class MultiplyGate(object):
   def forward(x, y):
      z = x*y
      return z
   def backward(dz):
      dx = self.y * dz
      dy = self.x * dz
      return [dx, dy]
```


강의에서는 총 3개의 assignment를 제공하는데 이에 대한 내용도 <br/>
추후에 포스팅 할 예정이다. <br/>

첫번째 과제에서 나오는 내용아 SVM과 Softmax를 작성하는 것인데, <br/>
여기서 오늘 배운 `forward` 와 `backward`  를 이용하면 된다. 

## Summary
- `forward` : 뉴럴 네트워크 모델의 입력층부터 출력층까지 순서대로 변수들을 계산하고 저장하는 것
- `backward` : 뉴럴 네트워크의 파라미터들에 대한 그래디언트(gradient)를 계산하는 방법<br/>
   > 입력값을 가지고 loss function에 대한 gradient를 계산하기위해 chain rule을 적용한다.

# Neural Networks
앞서 우리는 선형분류기에 대하여 배웠다.
$$f=Wx$$

이번 장에서는 2개의 layer를 가진 신경망에 대해 학습한다. 
$$ f = W_2max(0, W_1x) $$

![](/assets/image/2022-02-09-17-40-56.png)

${h = W_1x}$ 로 ${W_1}$의 score value를 의미한다. 이 값은 비선형이다. 

$$ f = W_3max(W_2max(0, W_1x)) $$
행렬곱 중간에 비선형 함수와 함께 선형 레이어를 여러개 쌓아 계산하는 방법으로 <br/>
layer의 개수를 확장해 나갈 수 있다. 

우리가 학습하고 있는 신경망 모델은 말 그대로 생명체의 신경망을 모티브로 하였다.

![](/assets/image/2022-02-09-17-45-06.png)

어느정도 생물학적 신경망과 비슷한 부분이 있지만, 그 기능이 정확히 일치하지는 않는다.<br/>
따라서 다음 사항을 주의하여야한다. 

 - 정말 다양한 구조가 있다.
 - Dendrite가 linear한 계산이 아닌 non-linear computations도 수행될 수 있다.
 - Synapses는 하나의 weight가 아니라 복잡한 dynamical한 system일 수 있다.
 - 우리가 activation function을 통해서 얻은 rate들이 적절하지 않을 수 있다.

## Activation functions

활성함수에 대해서는 추후에 다룰 예정으로, 강의에서는 잠깐 소개만 하고 넘어간다.
![](/assets/image/2022-02-09-17-48-00.png)


## Nueral Networks : Architecture

![](/assets/image/2022-02-09-17-49-16.png)

왼쪽 그림과 같은 구조를 `2-layer Nerual Net` 또는 `1-hidden-layer Neural Net`이라 한다.
오른쪽 그림과 같은 구조를 `3-layer Nerual Net` 또는 `2-hidden-layer Neural Net`이라 한다.

이때, 마지막 `hidden layer`와 `output layer` 를 연결하는 layer를 `Fully-Connected layer`라고 부른다. 


## Summary 

- 뉴런을 Fully-Conneted layers로 배열한다.
- layer의 `abstraction`은 좋은 속성을 가진다.
   > 행렬과 같이 vectorized code로 표현가능
- 신경망은 생물학적 신경망과 완전히 동일하지는 않다. 

