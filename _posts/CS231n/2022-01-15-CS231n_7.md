---
title : "CS231n 7강 Training NN(2)"
category :
    - CS231n
tag :
    - machine_learning
    - CS231n
    - computer vision
toc : true
toc_sticky: true
comments: true
---
신경망 학습에 대하여 알아보자


지난 시간엔 데이터 전처리에 대해 배웠다.

![](/assets/image/2022-02-18-22-32-11.png)

`normailization` 이전에는 손실함수가 아주 약간의 가중치 변화에도 예민하게 변화한다.

`normalization` 을 하게되면 데이터의 중심을 원점에 맞추고 (zero-cneter)

unit variance로 만들어주게 된다. 

이 경우 최적화가 더 쉽고 학습이 잘된다. 

오늘은 
- Fancier Optimization
- Regularization
- Transfer Leaning

에 대해서 배워보자

# Fancier Optimization


## SGD (Stochastic Gradient)
우선 가장 간단한 최적화 알고리즘을 살펴보자

```python
# Vanila Gradinet Descent

while True:
   weights_grad = evalute_gradient(loss_fun, data, weighs)
   weights += - step_size * weights_grad # perform parameter update 
```

바로 Stocahstic Gradient Descent 이다.

1. 미니배치 안의 data에서 loss를 계산
2. gradeint의 반대방향으로 weight 벡터를 update
3. 1,2를 반복하면 결국 아래 사진의 붉은색의 수렴하고 loss는 감소한다. 

![](/assets/image/2022-02-18-22-36-54.png)

해당 방법의 문제점에 대해서도 살펴보자.
- w1, w2가 있을때 둘중 하나는 update를 해도 손실 함수가 아주 느리게 변한다.
   > gradeint 방향이 고르지 못하기 때문에 지그재그 발생 <br/>
   ex) 수평축은 천천히 수직축은 급격히 변화 <br/>
    고차원으로 갈수록 이 현상은 더 심해진다. 

![](/assets/image/2022-02-18-22-38-17.png)

Q. loss가 한 방향으로는 빠르게, 다른 방향으론 느리게 변할경우 어떻게 될까?
 > 빠른 방향으로 학습이 가중되고 느린 방향으로는 학습이 덜 진행된다.

Q. loss function이 local minima 혹은 saddle point를 가지면 어떻게 될까?
 > 1. local minima 경우 oppostie gradient =0이 됨<br/>
> 2. saddle point 경우 한쪽 방향으로는 증가하고 다른 한쪽방향으론 감소한다.<br/>
> 고차원 공간에서 saddle point가 의미하는 것은 어떤 방향으론 loss가 증가하고 <br/>
> 몇몇 방향은 loss가 감소한다. 따라서 depp neural network는 local minimum보다 <br/>
> saddle point에 취약하다.
   
![](/assets/image/2022-02-18-22-48-32.png)
> saddle point 근처에서 gradient는 0이 아니지만 기울기가 아주 작다 <br/>
> gradient를 계산해서 update해도 기울기가 아주 작기 때문에 현재 가중치의 위치가 <br/>
> saddle point 근처라면 update는 아주 느리게 진행된다. 


SGD를 이용하여 손실함수 계산시 엄청 많은 training set 각각의 loss를 전부 계산해야 한다.

그래서 실제로는 미니배치의 데이터들만 가지고 loss를 추정한다. 

이 방법은 매번 정확한 gradient를 얻을수 없다. 추정한 값만 구할뿐이다.

따라서 noise가 존재하고 minima까지 도달하는데도 시간이 오래걸린다. 

![](/assets/image/2022-02-18-22-52-32.png)

## SGD + Momentum
**gradeint 계산시 velocity를 이용해보면 어떨까?**

위 아이딩로 나온 방법이 SGD에 모멘텀을 추가한 방식이다.

기존 SGD는
$$x_{t+1} = x_t - \alpha \nabla f(x_t)$$

``` python
while True:
   dx = compute_gradient(x)
   x += learning_rate * dx 
```

SGD + Momentum
$$v_{t+1} = \rho v_t + \nabla f(x_t) \\ x_{t+1} = x_t - \alpha v_{t+1}$$


```python
vx = 0
while True:
   dx = compute_gradient(x)
   vx = rho * vx + dx
   x += learning_rate * vx
```

여기서 rho 는 추가된 하이퍼파라미터 (모멘텀 비율을 의미)로

 보통 0.9와 같은 높은값으로 설정한다. 

SGD에 모멘텀을 추가하게 될 경우

공이 굴러 내려가는 상황과 유사하다고 볼 수 있다.

local minimum에 도달해도 여전히 velocity를 가지고 있기 때문에 

gradinet는 0이라도 움직일 수 있다.

![](/assets/image/2022-02-18-23-08-58.png)


## Nesterov Momentum
![](/assets/image/2022-02-18-23-09-39.png)

앞서 배운 모멘텀을 추가하는 방식은

현재지점에서 gradient를 계산후 veclocity와 결합하는 방식이다.

반면 `Nestrerov Momentum`은  우선 velocity 방향으로 움직이고

그 지점에서의 gradient를 계산하는 방식이다. 그리고 다시 원점으로 돌아가

그 둘을 결합한다.

vecloicty 방향이 잘못되었을 경우에는 

현재 gradeint 방향을 좀 더 활용할 수 있도록 해주는 방식이다.

하지만 Neural Network와 같은 non-convex problem에서는 성능을 보장할 수 없다.

$$ v_{t+1} = \rho v_t - \alpha \nabla f (x_t + \rho v_t) $$
$$ x_{t+1} = x_t + v_{t+1} $$

위 식에서 변수들을 적절히 바꾸면 loss와 gradient를 같은 점에서 계산가능하다.


${\hat{x_t} = x_t + \rho v_t}$$ 로 변형한뒤

$$ v_{t+1} = \rho v_t - \alpha \nabla f(\hat{x_t}) \\ 
   \hat{x_{t+1}} = \hat{x_t} - \rho v_t + (1 + \rho)v_{t+1}
$$

## AdaGrad

이 방법은 훈련 도중 계산되는 gradient를 활용하는 방법이다.

velocity term 대신 grad sqared term을 이용한다.

- 학습 도중에 계산되는 gradient에 제곱을 해서 계속 더해준뒤에

- update 할 때 update term을 앞서 계산한 gradeint 제곱항으로 나누어준다.

``` python
grad_squared = 0
while True:
   dx = compute_gradient(x)
   grad_sqaured += dx * dx 
   x -= learning_rate * dx / (np.sqrt(grad_sqaured) + 1e-7)
```

![](/assets/image/2022-02-18-23-49-40.png)

Q. condition number인 경우 (np.sqrt(grad_sqaured) + 1e-7) 값은?
 > A. small dimension에서는 gradient의 제곱 값 합이 작다. <br/>
 이 작은 값이 나눠지므로 가속도가 붙는다.<br/>
 large dimension에서는 gradient가 큰 값이므로 큰 값이 나눠진다. <br/>
 따라서 속도가 점점 줄어든다. 

 Q. Adagrad를 진행할 수록 어떤일이 발생하는가?
   > A. 값이 점점 작아진다. <br/>
   업데이트한  gradient의 제곱이 계속해서 더해지므로 이 값은 서서히 증가한다.<br/>
   이 증가한 값으로 나눠주게 되면 step size는 점점 더 작은 값이 된다.

   손실함수가 convex한 경우에 점점 작아지는 것은 좋은 특징이 될 수 있다.

   convex case에서는 minimum에 근접하면 서서히 속도를 줄여서 수렴하면 아주 좋다.

   하지만 non-convex case에서는 문제가 발생한다.

   saddle point에 걸려버렸을 때 AdaGrad는 점점 멈춰버릴 수 있다. 

   ## RMSProp
   
``` python
grad_squared = 0
while True:
   dx = compute_gradient(x)
   grad_sqaured = dexcay_rate * grad_squared + (1 - decay_rate) * dx * dx
   x -= learning_rate * dx / (np.sqrt(grad_sqaured) + 1e-7)
```

기존의 누적값에 dexcay_rate를 곱해주는 방식이다.

현재 gradeint 제곱은 (1-decay_rate)를 곱해주고 더해준다. 

보통 decay_rate는 0.9 또는 0.99를 사용한다.

## Adam

Adam은 가장 많이 쓰이는 방법이다.

``` python
first_moment = 0
second_moment = 0

while True:
   dx = compute_gradient(x)
   first_moment = beta1 * first_moment + (1 - beta1) * dx
   second_moment = beta2 * second_moment + (1 - beta2) * dx * dx
   x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)
```
   
first moment는 velocity를 담당한다.

second moment는 초기에 0으로 초기화하고, 1회 update 이후에도

여전히 0에 가깝다. update step에서 second moment로 나누면 초기 step이

엄청나게 커진다.

이 커진 step이 실제로 손실함수가 가파르기 때문이 아니라는것에 주의하자

이값은 seocnd moment를 0으로 초기화 시켰기 때문에 발생하는 인공적인 현상이다.

여기에 bais term을 추가하면 다음과 같다.

``` python
first_moment = 0
second_moment = 0

for t in range(1, num_itreations):
   dx = compute_gradient(x)
   first_moment = beta1 * first_moment + (1 - beta1) * dx
   second_moment = beta2 * second_moment + (1 - beta2) * dx * dx

   fist_unbias = first_moment / (1 - beta1 ** t)
   second_unbias = second_moment / (1 - beta2 ** t)

   x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)
```

실제로 Adam은 매우 성능이 좋다.

beta1 = 0.9, beta2 = 0.99로 설정하고 lr을 1e-3~1e-4 정도로만 설정하면

모든 아키텍쳐에서 잘 동작한다.

Adam을 이용하면 각 차원마다 적절하게 속도를 높이고 줄이면서

독립적으르ㅗ step을 조절한다.

손실함수가 각 방향으로 정렬되지 않고 기울어진 타원이라면

Adam은 차원에 해당하는 축만을 조절할 수 있다.

즉 **수평 수직축으로는 조절이 되나 회전은 불가능하다**

![](/assets/image/2022-02-19-00-02-32.png)


하이퍼파라미터로서 learning rate를 설정하는 다양한 방법이 존재한다.

1. step decay
   > 처음엔 learning rate를 높게 하고 <br/>
   나중엔 learning rate을 높게 하는 방법이다.

2. exponetial decay
3. 1/t decay

![](/assets/image/2022-02-19-00-04-26.png)

위 그림은 Resnet 논문에 있는 그림인데 현재 수렴을 잘 하고 있는 상황에서

gradient가 점점 작아질때 lr을 낮추어야 한다.

lr이 너무 깊은 깊게 갈수가 없기 때문이다. 

따라서 우리는

1. 우선 decay 없이 학습한다.
2. loss curve를 잘 살피고 있다가 decay가 필요한 곳이 어디인지 고려

다음 방법을 적용하면 된다. 

![](/assets/image/2022-02-19-00-05-54.png)

1. gradient 정보를 이용해서 손실함수를 선형함수로 근사시킨다. 
2. 이 1차 근사함수를 실제 손실함수라고 가정하고 step을 내려간다.
   > 하지만 이 근사함수로는 멀리 갈 수 없다.


![](/assets/image/2022-02-19-00-06-53.png)
좀더 나은 방법으로는 2차 근사 정보를 추가적으로 활용할 수 있다.

해당예시를 다차원으로 확장시키면 Newton step 이라고 한다. 


## L-BFGS

위 방법은 사실상 DNN에서는 잘 사용되지 않는 방법이다.

Stochastic case에서는 잘 동작하지 않는다.

non-convex problems에도 부적합하다.

## 정리

따라서 실제로 Adam을 가장 많이 사용한다.

하지만 full batch update가 가능하고 sthochasticity가 적은 경우라면

L-BFGS가 가장 좋은 선택이 될 수있다.

