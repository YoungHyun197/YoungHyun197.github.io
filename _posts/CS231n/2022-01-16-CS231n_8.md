---
title : "CS231n 8강 DeepLearning Software"
category :
    - CS231n
tag :
    - machine_learning
    - CS231n
    - computer vision
toc : true
toc_sticky: true
comments: true
---

# Image Classification

이미지 분류는 컴퓨터비전에 있어서 주요한 작업으로 <br/> 
input image에 대하여 어떤 category에 속하는지 결정하는 과정이다.<br/>

하지만 여러가지 문제점이 존재한다.<br/>
컴퓨터는 이미지를 볼때 사람과 같이 인식하는 것이 아니라 <br/>
여러개의 숫자들에 대한 격자판으로 인식하기 때문이다.(숫자들의 집합) <br/>
이것을 `sematic gap` 이라 일컫는다. <br/>

따라서 이미지 분류에서는 다음과 같은 변화에 상당히 취약하다. <br/>

1. Vewpoint varaition (카메라 위치 변화)
2. Illumination (조명에 의한 변화)
3. Deformation (다양한 자세등 객체 변형)
4. Occulision (가려짐에 의한 변화)
5. Background Clutter (배경과 유사한 색의 객체)
6. Intraclass variation (생김새, 크기, 색 등 클래스 내부 분산)

`Image Classification` 에는 크게 2가지 접근법이 있다.

> 1. edge등의 feature를 명시적 규칙집합으로 이해하는 방법
> 2. Data 중심으로 접근하는 방법

- 명시적 규칙집합에 단점
  1. 강인(robust) 하지 않음
  2. 다른 객체에 분류에 대한 확장성이 낮음


이러한 문제를 해결하기 위한 방법으로 data 중심 접근 방법이 등장했다.

- Data 중심 접근
  1. 이미지와 라벨에 대한 데이터셋을 수집
  2. 머신러닝을 이용하여 classifier를 학습
  3. 새로운 이미지에 대하여 classifier 성능 평가  


# K-Nearest Neighbor

`Image Classification`에서 사용하는 알고리즘 종류 중 하나이다.<br/>
해당 알고리즘은 크게 두 가지 step으로 구성된다.<br/>

1. `train` : 모든 데이터와 라벨을 기억
2. `predict` : 새로운 input에 대하여 training한 이미지와 가장 유사한 label을 예측

K-NN에서 주요한 파라미터는 크게 두 가지가 있다.

1. `K` : 이웃의 수
2. `Distance Metric` : 유사도를 측정하는 방식 (L1, L2등)

우선 K에 따른 알고리즘의 차이를 알아보자.

해당 알고리즘은 Distance Metrice을 이용해 가까운 K개의 이웃을 찾고<br/>
이웃끼리 투표하는 방법이다. 

K=1일때 살펴보면 `가장 가까운 이웃` 만을 보기 때문에 녹색 영역 가운데<br/>
노란색 영역이 발생하거나, 초록 영역이 파란영역을 침범하는 등의 문제가 발생한다.<br/>

K=3, 5로 늘리게 되면 결정 경계가 더 부드러워지고 더 좋은 결과를 보인다. <br/>
즉, K수를 높이면 잡음에 강해지는 것을 알 수 있다.


이때, 레이블링이 안된 흰색 지역은 <br/>
K-NN이 대다수를 결정할 수 없는 지역이다. 

![](/assets/image/2022-02-01-02-46-01.png)

이번에는 `Distnace Metrice` 에 대해 자세히 알아보자. 

강의에서 소개한 방법은 2가지이다. 이 외에도 다양한 방법이 존재한다.<br/>

1. `L1 distance` 단순히 두 이미지에 거리 차이를 합산하는 방법 <br/>
   `Manhattan distance` 라고도 부른다.<br/> 
   좌표계에 의존적인 data 사용시 사용<br/>
    ex) 각 직원 근속연수, 봉급 등 특징적인 요소

 <p align="center">


$${
d1(I_1, I_2) = \displaystyle\sum_{p}\left\lvert I_1^p - I_2^p\right\rvert
}$$

</p>

![L1 distance](/assets/image/2022-02-01-02-42-51.png)

2. `L2 distance` 두 픽셀간 차이의 제곱합의 제곱근을 씌우는 방법<br/>
   `Euclidean distance` 라고도 부른다.<br/>
   좌표계에 의존적이지 않을때 사용

<p align="center">


$${
d2(I_1, I_2) = \sqrt{\sum_{p}(I_1^p - I_2^p)^2}
}$$

</p>

아래 이미지를 통해 L1, L2 distance를 사용하는 상황을 시각적으로 확인 가능하다.<br/>

![](/assets/image/2022-02-01-03-00-08.png)
![](/assets/image/2022-02-01-03-03-33.png)

우리는 사전에 K값과 distance를 사용할지 등을 결정해주어야 하는데 <br/>
이러한 파라미터를 `Hyperparamter` 라고 부른다.<br/>
그렇다면 우리는 어떤 K값을 사용하는 것이 좋을까? <br/>

크게 3가지 방법에 대해서 알아보자. 

1. 학습데이터의 정확도와 성능을 최대화 하는 파라미터를 선택

   > 결과부터 말하자면 **절대 해서는 안되는 방법**<br/>
   > 학습데이터에 없던 새로운 데이터에 대해 좋은 성능을 보장할 수 없다. 
   > ![](/assets/image/2022-02-01-03-20-19.png)

2. 데이터를 `train`과 `test` set으로 나누어 학습시킨후 가장 성능이 좋은것을 선택

   > 이 방법 역시 **절대 해서는 안되는 방법**<br/>
   > 머신러닝의 궁극적인 목적은 한 번도 보지 못한 데이터에 대해서 잘 동작하는 것<br/>
   > 그저 `test set`에서만 잘 동작하는 하이퍼 파라미터를 고른 것일 수 있기 때문<br/>
   >  ![](/assets/image/2022-02-01-03-20-39.png)

3. 데이터를 `train`, `validation`, `test` set으로 분리후 `validation`에서 가장 좋은것을 선택 후<br/>
   `test`는 단 한 번만 수행하는 방법 

    > 해당 방법이 가장 권장되는 방법이고 실제로 많이 쓰인다.<br/>
    >  ![](/assets/image/2022-02-01-03-21-03.png)

4. **Cross-Validation** : `train` 데이터를 여러 부분으로 나눈 뒤 번갈아 가며 `validation`으로 지정<br/>
   `test` 데이터를 정해놓고 마지막에만 사용하는 방법.

   > 학습 자체가 계산량이 많기 때문에 딥러닝 같은 큰 모델에서는 잘 사용되지 않는다.
   >  ![](/assets/image/2022-02-01-03-23-28.png)


지금까지 우리는 K-Nearest Neighbor를 알아보았다.<br/>
하지만 K-NN은 이미지를 분류함에 있어서 **절대 사용되서는 안된다.**<br/>
그 이유는 다음과 같다.<br/>

1. 모든 train에 대한 정보를 가지고 있어야 하므로 매우 느리다.
2. `Distance Metric`은 픽셀단위에서 별로 유용한 정보가 아니다. (not informative)<br/>
   ![](/assets/image/2022-02-01-03-27-47.png)
   해당 이미지는 모두 같은 L2 distnace를 갖는다. 
3. 차원이 늘어나면 필요한 train data가 기하급수적으로 증가한다. (차원의 저주)




# Linear Classifcation

이제는 `Linear Classfiers`에 대해 알아보자. <br/>

`Linear classifiers` 는 쉽게 설명하자면 `레고 블럭`과 같다. <br/>
우리는 이 `레고 블럭` 들을 쌓아서 `Neural Network`를 생성할 수 있다. <br/>

앞서 배운 K-NN은 파라미터가 없다. (하이퍼 파라미터는 존재) <br/>
그저 `training set`을 가지고 있고, 모든 `training set`을 `test time`에 사용할 뿐이다.<br/>

하지만 `Parametric Approach`에서는 트레이닝 데이터의 정보를 요약한후 정보를 `W`에 모아준다. <br/>
이렇게 하게되면 더 이상 `test time`에서 `training set`이 필요하지 않다. 

![](/assets/image/2022-02-01-03-46-13.png)

해당 방법은 가장 쉬운 모델 설계방법으로 단순히 `W`와 `x`를 곱할 뿐이다. <br/>
일부 `bias` 를 더하는 경우도 존재한다. 이때는 **데이터와 무관하게** <br/>
특정 클래스에 **우선권을 부여**한다. 
ex) 고양이 데이터가 강아지 데이터 보다 많은 경우 - 고양이 클래스 bias 증가 

쉬운 예제를 한번 살펴보자.<br/>

      1. `input imasge`를 열벡터로 펴준다.
      2. 해당 문제에서는 임의로 정한 `W`가 존재한다. `W` 에 1번에서 만든 열벡터롤 내적해준다.<br/>

    (정확히는 W의 각행은 각 이미지에 대한 템플릿을 의미)<br/>
    여기서 내적은 클래스 간 템플릿의 유사도를 측정해주는 기능을 한다.<br/>

      3. 내적 결과에 `bias`를 더해준다.<br/>

    bias는 독립적인 데이터로 각 클래스에 scailing factor를 더해주는 것.<br/>

 ![](/assets/image/2022-02-01-03-53-38.png)

 하지만 `Linear Classification`에도 문제점이 존재한다. <br/>
 각 클래스에 대해서 단 하나의 템플릿만 학습가능하다는 점이다. <br/>
 이렇게 될 경우 풀수 없는 여러가지 문제들이 존재한다. <br/>

![](/assets/image/2022-02-01-03-56-59.png)

비록 `Linear Classifcation`이 풀지 못하는 다양한 문제들이 존재하지만, <br/>
아주 쉽게 이해하고 해석할 수 있는 알고리즘 이라는데 의의가 있다. <br/>

우리는 지금까지 `Linear Classifier`가 어떻게 생겼고 어떻게 동작하는지를 배웠다 <br/>
아직 어떻게 `W`를 구할수 있는지는 배우지 않았다.<br/>
다음 시간에는 `Loss function`, `Optimization`, `CNN`에 대해 배울 예정이다.  

# Question

1. N개의 examples을 가지고 있을때, NN-classifier의 train, prediction 함수의 시간복잡도는?

   > `Train` O(1), `Predict` O(N) <br/>
   > predict단계에서 N개의 학습데이터 전부를 테스트이미지와 비교해야만 하므로<br/>
   > 상당히 느리다. 우리는 train time은 느리더라도 test time은 빠르길 원한다.<br/>

2. 최적의 하이퍼 파라미터를 찾을때 까지 학습을 다시 시키는 것은 흔한 방법인가?

   > 그때 그때 다르다. 하지만 시간적 여유가 있을때는 권장



# Summary

## K-NN

1.  `Image Classification`은 입력 이미지에 대하여 해당 이미지가 속하는 catergory를 찾는 기법<br/>
    `training set`은 image와 label을 가지고, `test set`은 input의 label을 예측한다. 
2.  `K-Nearest Neighbors` classifier는 `training set` 중에서 가장 가까운 샘플을 골라서 label을 예측
3.  K-NN에서 하이퍼 파라미터는 `Distance Metric`과 `K` 이다.
4.  `L1 (Manhattan)`는 특징에 의존적일 때 사용, `L2 (Eucleadean)`는 특징에 의존적이지 않을 때 사용
5.  하이퍼 파라미터를 결정할 때, `Validation set` 을 설정하고, `Test set`은 마지막에 오직 1번만 사용한다.

## Linear Classification  

1. `Linear Classifier`는 `레고 블럭`에 비유가능. `레고 블럭`을 쌓아서 `Nerual Network` 구성

2. 수식으로 다음과 같이 표현 가능하다.

   <p align="center"> 
   ${ 
      f(x, W) = Wx + b
   }$ 
   </p>

3. `x`는 input 이미지, `W`는 이미지에 대한 요약된 정보. `Wx`는 클래스 간 템플릿의 유사도를 측정하는 연산

4. 각 클래스에 대해서 단 하나의 템플릿만 학습하므로 풀지 못하는 여러 문제가 존재

5. 아주 쉽게 이해하고 해석할 수 있는 알고리즘 이라는 의의가 있다. 