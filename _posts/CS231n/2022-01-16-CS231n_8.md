---
title : "CS231n 8강 DeepLearning Software"
category :
    - CS231n
tag :
    - machine_learning
    - CS231n
    - computer vision
toc : true
toc_sticky: true
comments: true
---
Deep-Learning Framework에 대하여 알아보자


딥러닝 프레임워크를 사용하는 이유는 다음과 같다.

1. Computational Graph를 쉽게 build
2. Gradient 을 쉽게 계산가능
3. GPU에서 효과적으로 딥러닝을

딥러닝 프레임워크에는 크게 2가지가 존재한다.

1. Tensorflow
2. Pytorch

오늘 이에 대해 알아보고 비교해보는 시간을 가져보자.


최근 많이 쓰이고 있는 딥러닝 프레임워크인 Tensorflow부터 한 번 알아보자.

# Tensorflow

tensorflow에서는 Neural Network를 먼저 정의하고 그 다음 run을 통해 training을 시켜야 한다.

Graph가 굉장히 static하다는 특징을 가지고 있다.

static graph의 특징을 한 번 살펴보면, 한번 model을 구성해 놓으면 재사용이 쉽고, run을 하기 전에 최적화 과정을 진행할 수 있다.



처음에 Neural Net을 만든다. placeholder라는 곳에 input값과 weights들의 값을 넣어준다.

Xavier에 의해 weight의 초기화 값을 설정해준다. 

그리고 y_pred 를 이용해 자동적으로 weight값이 바뀌도록 설정해준다.

그리고 optimizer를 이용해 gradient값을 계산하고,
loss값이 최소가 되도록 계속 update를 해준다.

with tf.Session() as sess:
부분 이후에 실제 data값을 넣어주고, training을 시키는 과정이다.




# Pytorch

## Tensor

텐서는 배열이나 행렬과 매우 유사한 자료구조이다.
Pytorch에서 텐서를 사용하여 모델 입출력 뿐만 아니라 매개변수를 부호화한다.

GPU 등 특수한 하드웨어에서 실행가능 하다는 점을 제외하면 numpy의 ndarray와 매우 유사하다.

## Varaible

variable 객체는 그래프의 노드라고 할 수 있다.<br/>
그래프를 구성하고 그레디언트 등을 계산 가능하다. 

## Module

Moudule 객체를 이용해서 Neural Network를 구성할 수 있다.

## Pytorch vs TensorFlow

Pytorch의 Tensor = Tensorflow의 Numpy array</br>
Pytorch의 Variable = Tensorflow의 Tensor, Variable, Placeholder </br>
Pytroch의 Module = Tensorflow의 tf.layersm or TFSlim, or TFLearn, or Sonnet, or ...</br>
<br/>
`Pytorch`는 고수준의 추상화를 이미 내장하고 있다. Tensorflow처럼 어떤 모듈을 선택할 지 고민할 필요가 없음 <br/>

**가장 큰 차이점은 Pytorch의 tensor는 GPU에서도 돌아간다는 것이다.** 

## Tensor 예시

Pytorch의 tesnor는 Numpy의 array와 유사하다<br/>
실제 numpy array를 사용하지는 않고 Pytorch tensor를 이용한다<br/>

```python
import torch

dtype = torch.FloatTensor

# Create random tensors for data and weights
N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in).type(dtype)
y = torch.randn(N, D_out).type(dtype)
w1 = torch.randn(D_in, H).type(dtype)
w2 = torch.randn(H, D_out).type(dtype)

learning_rate = 1e-6
# Forward pass : compute predictions and loss
for t in range(500):
    h = x.mm(w1)
    h_relu = h.clamp(min=0)
    y_pred = h.relu.mm(w2)
    loss = (y_pred - y).pow(2).sum()

    # Backward pass : manually compute gradients
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h.relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h[h < 0] = 0
    grad_w1 = x.t().mm(grad_h)
    
    # Gradient descent step on weights
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2

```

해당 코드를 GPU에서 실행시키리면 dtype 부분을 수정해주면 된다.<br/>


```python
dtype = torch.cuda.FloatTensor 
```

FloatTensor 대신에 cuda.FloatTensor을 사용하면 GPU에서 실행가능하다.<br/>
Pytorch의 `tensor`는 쉽게 생각해서 `Numpy` + `GPU` 라고 생각하면 된다.<br/>
<br/>

## Varaible 예시

variable은 computational graphs를 만들고 이를 통해 그레디언트를 자동으로 계산하는 목적으로 이용<br/>
`x`는 variable이고 `x.data` 는 tensor이다. `x.grad`도 variable로 Loss에 대한 gradient를 담고있다<br/>

```python
import torch
from torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in), requires_grad=False)
y = Variable(torch.randn(N, D_out), requires_grad=False)
w1 = Variable(torch.randn(D_in, H), requires_grad=True)
w2 = Variable(torch.randn(H, D_out), requires_grad=True)
```
