---
title : "CS231n 6강 Training NN(1)"
category :
    - CS231n
tag :
    - machine_learning
    - CS231n
    - computer vision
toc : true
toc_sticky: true
comments: true
---
신경망 학습에 대하여 알아보자

# One time setup
## Activation Functions
우리가 앞서 배운 신경망은 다음과 같은 순서로 동작했다.
1. 입력을 가중치와 곱 ex) FC, CNN
2. 활성함수(비선형 연산)을 거침

이번 시간엔 활성함수에 대해 보다 자세히 알아보자.

활성함수의 종류는 다양하다. 
- Sigmoid
- tanh
- ReLU
- Leaky ReLU
- Maxout
- ELU
- etc.


![](/assets/image/2022-02-14-20-40-58.png)

### Sigmoid 
우선 Sigmoid 함수를 살펴보자

![](/assets/image/2022-02-14-20-41-36.png)

$$ \sigma(x) = \frac{1}{1+e^{-x}}$$

- 시그모이드 함수는 [0, 1] 의 값을 취한다.
- 함수의 양끝단을 제외하면 선형과 유사하다.
  
과거엔 많이 사용되었지만 사용되지 않는 이유로는 단점이 많기 때문이다.
1. Saturated Neuron "Kill Gradient" => 그레디언트 소멸문제
   > ![](/assets/image/2022-02-14-20-48-50.png) <br/>
   x= -10 또는 10일때는 gradeint가 소멸한다. <br/>
   x = 0일때는 정상동작한다.  <br/> <br/>
즉, 함수의 flat한 부분에서 gradient가 소멸한다.

2. 출력이 not zero centered 
   > ![](/assets/image/2022-02-14-20-51-19.png) <br/>
   입력 x가 모두 양수/음수로 이루어지면 파라미터 update시 다같이 <br/>
   증가하거나 감소하기만 한다. 이것은 아주 비효율적이다. <br/>
   우측그림에서 파란색이 최적의 업데이트 방향이지만, <br/>
   모두 양수/음수일 경우 빨간색 화살표처럼 지그재그로 움직이게 된다. <br/>
   <br/>
   이것이 바로 `zero-mean` data를 사용하는 이유이다.
3. exp() 계산 비용이 크다 (큰 문제점은 아님)


### tanh
이번엔 tanh 함수에대해 알아보자  <br/>

![](/assets/image/2022-02-14-20-56-05.png)

- [-1, 1] 범위의 값을 갖는다.
- zero-centerd를 만족한다. 
- 여전히 gradeint 소멸문제가 발생한다. 
   > 양 끝단에서 flat한 부분 존재 

### ReLU
![](/assets/image/2022-02-14-21-10-03.png)
$$ f(x) = max(0, x) $$

- 입력이 음수면 0이되고 양수면 그대로 출력된다. 
- 양의 영역에선 saturate 되지 않는다.
  > ReLU의 가장 큰 장점
- 계산이 아주 효율적이다.
  > 단순히 max연산이므로 계산이 매우 빠름 <br/>
  > tanh 보다 수렴속도가 6배 빠름 <br/>
- 생물학적 타당성이 sigmoid보다 크다.
- AlexNet(2012)에서 처음으로 사용되었다. 
- 음의 영역에서는 여전히 gradient 소멸
  > ![](/assets/image/2022-02-14-21-13-34.png) <br/>
  > x = -10일땐 소멸, x=0일땐 정의되진 않지만 실제론 0, x=10 정상동작<br/><br/>
  > ![](/assets/image/2022-02-14-21-14-45.png) <br/>
  > - `activ ReLU` : 양의 부분만 활성화 <br/>
  > - `dead ReLU` : 음의부분을 활성화 되지않음 (Data CLoud에서 떨어져있는 경우)<br/>
      > 발생원인은 <br/>
      1. 초기화를 잘못한 경우 : 가중치 평면이 data cloud에서 멀리 떨어짐 <br/>
      2. Learning Rate가 지나치게 큰 경우 : 처음에는 학습이 잘되다가 죽어버림<br/><br/>
   ReLU를 초기화 할때 Positive bias를 추가해줌으로써 <br/>
   update시에 active ReLU가 될 가능성을 높이는 방법이 존재한다. <br/><br/>
   하지만 도움이 된다는 의견도 있고 그렇지 않다는 의견도 존재한다.<br/>
   따라서 일반적으로는 zero-bias로 초기화한다. 

### Leaky ReLU
![](/assets/image/2022-02-14-21-22-19.png)
$$ f(x) = max(0.01x, x) $$

- saturate 되지 않는다.
- 계산이 효율적이다.
- sigmoid, tanh보다 훨씬 빠르게 수렴한다.
- gradient가 소멸되지 않는다.
  > negative 영역에서 기울기를 살짝 주게되면 gradient 소멸을 상당부분 해결가능 <br/><br/>
  즉, deae ReLU 해결가능 

### Parametric Rectifier (PReLU)
$$ f(x) = max(\alpha x , x) $$

- 기울기 알파를 파라미터로 결정. 이때, 알파는 backprop으로 학습

### Exponential Linear Units (ELU)
![](/assets/image/2022-02-14-21-25-40.png)

- Leaky ReLU와 ReLU의 중간
- zero-mean에 가까운 출력
- Saturation 발생
  > saturation이 좀 더 잡음에 강인할 수 있다는 주장

### Maxout "Neuron"

$$ max(w_1^Tx + b_1, w_2^Tx+b_2)$$

- 입력을 받아들이는 특정한 기본형식 지정 x 
- 2개의 선형함수를 일반화 (ReLU와 Leaky ReLU)
- Saturation 되지 않아 gradeint가 죽지 않는다.
- 뉴런당 파라미터의 수가 2배가 되는 단점 존재


실제로는 주로 ReLU를 사용한다. 이때 learning rate 설정에 주의해야한다.
- Leaky ReLU / Maxout / ELU도 쓸만하지만 아직은 실험단계
- tanh는 써볼만하지만 너무 기대하지는 말자. 
- sigmoid는 사용하지 말자.

## Preprocessing


## Weight Initialization
## Batch Normalization
## Regularization
## Gradient Checking

# Training dynamics
## Babysitting the Learning Process
## Parameter Updates
## Hyperparameter Optimization

# Evalutation
## Model Ensembles