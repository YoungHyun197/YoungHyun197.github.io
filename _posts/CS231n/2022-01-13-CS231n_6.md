---
title : "CS231n 6강 Training NN(1)"
category :
    - CS231n
tag :
    - machine_learning
    - CS231n
    - computer vision
toc : true
toc_sticky: true
comments: true
---
신경망 학습에 대하여 알아보자



# One time setup
## Activation Functions
우리가 앞서 배운 신경망은 다음과 같은 순서로 동작했다.
1. 입력을 가중치와 곱 ex) FC, CNN
2. 활성함수(비선형 연산)을 거침

이번 시간엔 활성함수에 대해 보다 자세히 알아보자.

활성함수의 종류는 다양하다. 
- Sigmoid
- tanh
- ReLU
- Leaky ReLU
- Maxout
- ELU
- etc.


![](/assets/image/2022-02-14-20-40-58.png)

### Sigmoid 
우선 Sigmoid 함수를 살펴보자

![](/assets/image/2022-02-14-20-41-36.png)

$$ \sigma(x) = \frac{1}{1+e^{-x}}$$

- 시그모이드 함수는 [0, 1] 의 값을 취한다.
- 함수의 양끝단을 제외하면 선형과 유사하다.
  
과거엔 많이 사용되었지만 사용되지 않는 이유로는 단점이 많기 때문이다.
1. Saturated Neuron "Kill Gradient" => 그레디언트 소멸문제
   > ![](/assets/image/2022-02-14-20-48-50.png) <br/>
   x= -10 또는 10일때는 gradeint가 소멸한다. <br/>
   x = 0일때는 정상동작한다.  <br/> <br/>
즉, 함수의 flat한 부분에서 gradient가 소멸한다.

2. 출력이 not zero centered 
   > ![](/assets/image/2022-02-14-20-51-19.png) <br/>
   입력 x가 모두 양수/음수로 이루어지면 파라미터 update시 다같이 <br/>
   증가하거나 감소하기만 한다. 이것은 아주 비효율적이다. <br/>
   우측그림에서 파란색이 최적의 업데이트 방향이지만, <br/>
   모두 양수/음수일 경우 빨간색 화살표처럼 지그재그로 움직이게 된다. <br/>
   <br/>
   이것이 바로 `zero-mean` data를 사용하는 이유이다.
3. exp() 계산 비용이 크다 (큰 문제점은 아님)


### tanh
이번엔 tanh 함수에대해 알아보자  <br/>

![](/assets/image/2022-02-14-20-56-05.png)

- [-1, 1] 범위의 값을 갖는다.
- zero-centerd를 만족한다. 
- 여전히 gradeint 소멸문제가 발생한다. 
   > 양 끝단에서 flat한 부분 존재 

### ReLU
![](/assets/image/2022-02-14-21-10-03.png)
$$ f(x) = max(0, x) $$

- 입력이 음수면 0이되고 양수면 그대로 출력된다. 
- 양의 영역에선 saturate 되지 않는다.
  > ReLU의 가장 큰 장점
- 계산이 아주 효율적이다.
  > 단순히 max연산이므로 계산이 매우 빠름 <br/>
  > tanh 보다 수렴속도가 6배 빠름 <br/>
- 생물학적 타당성이 sigmoid보다 크다.
- AlexNet(2012)에서 처음으로 사용되었다. 
- 음의 영역에서는 여전히 gradient 소멸
  > ![](/assets/image/2022-02-14-21-13-34.png) <br/>
  > x = -10일땐 소멸, x=0일땐 정의되진 않지만 실제론 0, x=10 정상동작<br/><br/>
  > ![](/assets/image/2022-02-14-21-14-45.png) <br/>
  > - `activ ReLU` : 양의 부분만 활성화 <br/>
  > - `dead ReLU` : 음의부분을 활성화 되지않음 (Data CLoud에서 떨어져있는 경우)<br/>
      > 발생원인은 <br/>
      1. 초기화를 잘못한 경우 : 가중치 평면이 data cloud에서 멀리 떨어짐 <br/>
      2. Learning Rate가 지나치게 큰 경우 : 처음에는 학습이 잘되다가 죽어버림<br/><br/>
   ReLU를 초기화 할때 Positive bias를 추가해줌으로써 <br/>
   update시에 active ReLU가 될 가능성을 높이는 방법이 존재한다. <br/><br/>
   하지만 도움이 된다는 의견도 있고 그렇지 않다는 의견도 존재한다.<br/>
   따라서 일반적으로는 zero-bias로 초기화한다. 

### Leaky ReLU
![](/assets/image/2022-02-14-21-22-19.png)
$$ f(x) = max(0.01x, x) $$

- saturate 되지 않는다.
- 계산이 효율적이다.
- sigmoid, tanh보다 훨씬 빠르게 수렴한다.
- gradient가 소멸되지 않는다.
  > negative 영역에서 기울기를 살짝 주게되면 gradient 소멸을 상당부분 해결가능 <br/><br/>
  즉, deae ReLU 해결가능 

### Parametric Rectifier (PReLU)
$$ f(x) = max(\alpha x , x) $$

- 기울기 알파를 파라미터로 결정. 이때, 알파는 backprop으로 학습

### Exponential Linear Units (ELU)
![](/assets/image/2022-02-14-21-25-40.png)

- Leaky ReLU와 ReLU의 중간
- zero-mean에 가까운 출력
- Saturation 발생
  > saturation이 좀 더 잡음에 강인할 수 있다는 주장

### Maxout "Neuron"

$$ max(w_1^Tx + b_1, w_2^Tx+b_2)$$

- 입력을 받아들이는 특정한 기본형식 지정 x 
- 2개의 선형함수를 일반화 (ReLU와 Leaky ReLU)
- Saturation 되지 않아 gradeint가 죽지 않는다.
- 뉴런당 파라미터의 수가 2배가 되는 단점 존재


실제로는 주로 ReLU를 사용한다. 이때 learning rate 설정에 주의해야한다.
- Leaky ReLU / Maxout / ELU도 쓸만하지만 아직은 실험단계
- tanh는 써볼만하지만 너무 기대하지는 말자. 
- sigmoid는 사용하지 말자.

## Preprocessing

![](/assets/image/2022-02-15-17-27-06.png)

기계학습에서는 주로 orginal data를 2가지 전처리를 거친다.

우선 zero-mean으로 만들어주고, 그 다음에 normalize를 해준다.

그렇다면 normalize는 왜 하는 걸까?

normlaize를 하면 모든 차원이 동일한 범위안에 있게 해주기 때문에

각각의 데이터가 모두 동등한 기여를 하게 된다. 

예를 들면 전부 + 또는 -이면 문제가 발생할 수 있는데 

이것을 방지하는 차원이다. 

앞서 배운 zero-mean이 아닌 데이터의 zig zag path를 생각하면 이해하기 수월하다.

하지만 이미지 처리의 경우 zero-centreing 정도만 수행한다.

이미지가 이미 각 차원간의 scale이 어느정도 맞춰져 있기 때문이다.

강의 내용중 한 학생이 이러한 질문을 했다.

Q. Train 단계에서 전처리를 수행하면 Test 단계에서도 전처리를 해줘야 하나?
 > A. Yes, 일반적으로 train에서 제안한 평균을 test data 에도 동일하게 적용한다.

Q. 전처리가 sigmoid problem을 해결가능한가?
 > A. sigmoid의 zero-mean 문제를 단지 "첫번째 레이어" 에서만 해결 가능

이미지 처리에서는 

- 일반적으로 네트워크에 들어가기 전에 평균값을 빼준다.
- 실제로 일부 네트워크는 채널 전체의 평균을 구하지 않고 채널마다 <br/>
  평균을 독립적으로 계산하는 경우도 존재한다.
- 입력단계에서 채널은 RGB를 의미한다.
- 평균 계산하는 방법
  > train data 전부를 mini batch 단위로 학습하더라도 <br/>
  평균계산은 mini batch단위로 각각이 아니라 전체로 계산한다. 

## Weight Initialization

![](/assets/image/2022-02-15-17-35-54.png)

모든 가중치가 0이면 어떤 일이 발생할까?

모든 가중치가 똑같은 값으로 update된다.

결국 모든 뉴런이 똑같이 생기게 된다.

그렇다면 가중치는 어떻게 초기화 해야할까?

1. small random numbers
   > 임의의 작은값으로 초기화. 이때 초기 w를 표준 정규분포에서 샘플링 <br/>
   하지만 깊은 netwrok에서 문제가 발생한다. <br/><br/>
  ![](/assets/image/2022-02-15-17-38-56.png) <br/> <br/>

    평균은 항상 0근처에 위치하지만 <br/>
    표준편차는 가파르게 줄어들며 0이된다. <br/>
    W를 곱할수록 W가 너무 작은 값들이라서 출력값이 급격히 줄어든다<br/><br/>

    결국 0이되고 모든 활성함수 결과가 0이된다. 

    backward pass에 관해 생각해보자.<br/>

    `local gradient` * `upstream gradient` 에서 <br/>
    Wx를 W에 대해 미분시 local gradient = x 이다 <br/>
    x가 엄청 작은 값이기 때문에 gradeint도 작아진다. <br/>
    결국 update가 잘 일어나지 않는다. <br/>

    forwar pass와 마찬가지로 W가 계속 곱해지며 gradient가 점점 작아지는 것이다.<br/>


    ![](/assets/image/2022-02-15-17-46-18.png)

    random 초기화시 기존에 np.random.randn(fan_in, fan_out) * 0.01 이 아닌<br/>
    1.0을 곱하게 되면 어떨까?

    가중치가 큰 값을 가지므로 tanh의 출력은 항상 saturation 된다.

    출력이 항상 -1이거나 +1이 되므로 gradient는 결국 0이되고<br/>
    가중치 update가 일어나지 않는다. 

    이처럼 적절한 가중치를 얻는것은 너무 어렵다.<br/>
    너무 작으면 사라져버리고 너무 크면 saturation 되어버린다. 

    이 와중에 합리적인 방법이 제안되었다. 

    Xavier initialization으로 불리는 방법으로 아래와 같이 표현가능하다. 

    W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)

    바로 standard gaussian으로 뽑은 값을 "입력의 수"로 스케일링 하는 것이다.<br/>
    이 방법은 기본적으로 입/출력의 분산을 맞춰주자는 아이디어에서 제안되었다. <br/>
    직관으로 보면 입력수가 작으면 작은값, 크면 큰값으로 나누는 것이다. <br/>
    왜냐하면 입력수가 작으면 큰 W가 필요하고, 크면 작은 W가 필요하기 때문이다. <br/>

    따라서 각 layer의 입력이 unit gaussian으로 맞춰진다.

    하지만 위 식의 가정인 inear region이 있다고 가정하기 떄문에<br/>
    ReLU에선 정상동작하지 않는다. 

    위 식은 2015년에 개선되었는데 

    W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)

    로 사용시 성능이 향상됨을 발견할 수 았다.


## Batch Normalization

배치정규화는 gaussian의 범위로 activation을 유지시키는 것에 관련한 또다른 아이디어이다.

**layer의 입력이 unit gaussian이 되도록 강제**하는 방법이다. 

우리는 어떤 layer로 부터 나온 batch 단위 만큼의 activations이 있을때 <br/>

이 값들이 unit gaussian이길 원한다.

현재배치에서 사용한 mean과 varaince를 이용해서 normalize 할 수 있다.

가중치 초기화 대신 학습할 때 각 레이어에서 위 과정을 거치며 <br/>

모든 layers가 unit gaussian이 되도록 하는 것이다.

즉, 각 뉴런을 평균과 분산으로 Normalization 해주는 함수를 구현하면 되고<br/>

해당 함수는 아래와 같다.

$$ \hat{x} ^{(k)} = \frac{x^{(k)}- E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}} $$

우리는 평균과 분산을 "상수"로 가지고만 있으면 

언제든지 미분가능하며 back prop이 가능해진다.

![](/assets/image/2022-02-15-18-00-30.png)

배치정규화는 총 2가지 step으로 구성된다.

1. 각 차원의 독립적으로 평균과 분산을 계산한다.
2. 정규화 식을 적용

batch당 N개의 학습 data가 존재

각 data는 D차원이다.

이때 깍 차원별로 평균을 구해준 뒤에 한 batch내에 전부 계산하며 normalize하는것.

![](/assets/image/2022-02-15-18-01-45.png)

Batch Noramlization은 `FC` 직후, `activate` 직전에 시행한다.

이렇게 해주면 Normalization이 bad effect를 상쇄 시킬 수 있다. 

BN은 입력의 scale 만 살짝 조정해주는 역할이므로 FC와 Conv 어디든 적용 가능하다.

다만, Conv Layer와의 차이점은 

Normalization을 각 차원마다 독립적으로 수행하는 것이 아니라

같은 activation Map의 같은 채널에 있는 요소들은 같이 normalize 해준다.

즉 , conv Layer는 Activation map마다 평균과 분산을 하나만 구한다.

우리가 정말 바라는 것이 tanh의 입력이 unit gaussian 인지 고민해보자.

normalization이 하는 일은 바로 입력이 tanh의 linear한 영역에만 존재하도록 

강제하는 것이다. 그렇게 되면 saturation이 전혀 발생하지 않게 된다.

하지만, saturation이 전혀 일어나지 않는것보다는

saturation이 얼마나 일어날지를 조절하는 것이 바람직하다. 

![](/assets/image/2022-02-15-18-09-23.png)

우리는 앞서 배치정규화의 식을 살펴봤다.

$$ \hat{x} ^{(k)} = \frac{x^{(k)}- E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}} $$

위 식이 보기엔 어려워 보이지만 알고보면 매우 단순하다.

단지 입력에서 평균을 빼고 표준편차로 나눈것이다.

이 식을 활용하면 우리는 normalize 된 값을 다시 원상복구도 가능하다.



사실상 **BN은 선택이 아닌 필수**이다. 

정리하면 입력이 있고 mini-batch에서 평균을 계산한다.

모든 mini-batch마다 각각 계산을 하고, 분산도 계산해준다.

평균과 분산으로 normalize 한 이후에 

다시 추가적인 sacling, shifting factor를 사용해준다.

따라서 BN은 결국 **gradient 흐름을 원활히 해주어 학습이 잘되게** 해준다.


# Training dynamics
## Babysitting the Learning Process

학습과정을 어떻게 모니터링 하는지 알아보자.

데이터를 전처리한 뒤에 우리는 네트워크 모델구조를 선택하게된다.

네트워크를 초기화할때 forrward pass후 loss가 그럴듯한지 확인해야한다.

데이터를 일부만 우선 학습한뒤에
- 데이터가 적으면 overfit이 발생할 것
- loss가 많이 감소
- 이때는 regularization 사용 x (loss 감소 확인을 위해)
- loss가 0을 향해 꾸준히 내려가는지 확인 
- 이후 전체 데이터 셋을 사용하여 regularizatio을 약간 주며 적절한 lr을 찾는다.
- lr이 너무 작으면 loss가 잘 줄어들지 않는다.

위 로직을 이용해서 우리는 네트워크를 초기화할 수 있다.

Q.만약 loss가 크게 줄어들지 않았음에도 Accuracy가 일정부분까지 증가한다면 그이유는?
 > A. 가중치는 서서히 변하는 중이다. Accuracy는 그저 가장 큰 값만 취하므로<br/>
 Accuarcy가 크게 뛸 수도 있는것이다.

보통 lr은 1e-3 ~ 1e-6를 사용한다. 

## Hyperparameter Optimization
하이퍼 파라미터 최적화 방법에 대해 알아보자.

`Cross-validation` stratgey는

 training set으로 학습하고 validation set으로 평가하는 방법이다. 

 1. coarse stage 
   > validation set으로 평가 <br/>
   > 하이퍼 파라미터 최적화시에는 log scale로 값을 주는 것이 좋다.<br/>
   > 10의 차수값만 sampling하자.  (-3 ~ -6) <br/>
   > leraning rate는 gradient와 곱해지기 때문에 큰 폭으로 변화하는 것을 방지할 수 있다.<br/>
![](/assets/image/2022-02-16-17-27-36.png)

 2. fine stage
  >![](/assets/image/2022-02-16-17-30-12.png)
  해당그림에선 14번째가 가장 좋아보인다. 하지만 문제점이 있다. <br/>
  그 이유는 최적의 lr을 효율적으로 탐색할 수 없을수도 있기 때문이다.<br/>
  실제 최적값이 1e-5 ~ 1e-6 사이에 있을수도 있다.<br/>
  즉, 범위 이동시 최적값을 찾을수도 있다.

탐색방법은 어떤것을 사용하는 것이 좋을까?

![](/assets/image/2022-02-16-17-32-37.png)

크게 `Grid Search` 와 `Random Search`가 존재하는데

`Grid Search` 의 경우 hyper parameter를 고정된 값과 간격으로 sampling 한다.

우리는 주로 `Random Search` 를 사용하는데 그 이유는 다음과 같다.
- 내 모델이 어떤 특정  파라미터의 변화에 민감하게 반응할 경우 
- 해당 함수가 더 비효율적인 dimetionality를 보인다고 판단 가능
- 따라서 random search를 통해 중요한 파라미터에게 많은 sampling 가능
- 즉, important variable에서 더다양한 값을 샘플링 할 수 있다. 

cross-validation을 사용시 우리는 최적의 하이퍼파라미터를 찾기 위해

다양한 값을 직접 돌려보고 모니터링하면서 어떤 값이 좋고 나쁜지를 확인해야한다.

- loss가 발산하는 경우 => lr 이 너무 높음
- loss가 너무 linear 한 경우 => lr이 너무 낮음
- loss가 가파르게 내려가다가 정체 => 여전히 너무높음, 
  >lr이 너무 크게 점프하여 local optimal에 도달 못하는경우

최적의 learning rate는 다음과 같이 나타난다.


![](/assets/image/2022-02-16-17-38-04.png)

![](/assets/image/2022-02-16-17-38-22.png)

위 그림처럼 loss가 평평하다가 갑자기 가파르게 감소하는 경우엔

초기화시 문제가 있을 가능성이 높다

gradient의 backprop이 초기에는 잘 되지 않다가 학습이 진행되면서 회복되는 경우이다.

![](/assets/image/2022-02-16-17-39-10.png)

train_acc와 val_acc이 큰 차이를 보이는 경우

overfit을 의심할 수 있다.

위 경우 rgualrization 강도를 높이는 방법을 시도해 볼 수 있다.

만약에 차이가 없다면 아직 overfit하지 않은것이므로 

capacity를 높힐수 있는 충분한 여유가 있다고 판단 가능.

```python
# assume parameter vector W and it gradient vecotr dW
param_scale = np.linalg.norm(W.ravel())
update = -lr * dW # Simple SGD update
update_scale = np.linalg.norm(update.ravle())
W += update # the actual update
print(update_scale / param_scale)
```

가중치 크기대비 가중치 업데이트의 비율을 지켜보는 것이 핵심이다.

1. 파라미터의 norm 을 구해서 가중치 규모를 계산
2. update 사이즈 또한 norm을 통해 구할 수 있고, 이를 통해 얼마나 크게
update 되는지 알 수 있다. <br/> 우리는 대략 이 비율이 0.001이 되길 원한다.
# Summary

- 활성화 함수로는 **ReLU**를 사용하자
- Data Processing 은 이미지의 경우 **subtract mean** 사용
- 가중치 초기화의 경우 **Xaiver init** 사용
- 배치정규화는 항상 사용하자.
- Learning Process를 지켜보자
- 하이퍼 파라미터를 최적화하자. random sample 한후 적절한 경우에 log scale로 변화