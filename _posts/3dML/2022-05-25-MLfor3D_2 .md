---
title : "Point Cloud Processing"
category :
    - 3dML
tag :
    - machine_learning
    - CS231n
    - computer vision
    - 3d data
toc : true
toc_sticky: true
comments: true
---
Point Cloud Processing에 대해 알아보자.

# 복습
지난 시간에 배운 내용을 살펴보면 

Voxels는 3D data의 sparsity를 다룰할 필요가 있다. 

Points Cloud의 장점은 생성이 쉽고 단점은  메모리 효율이 나쁘고, render/texture 불가능한 것이다. 

Mesh의 장점은 메모리 효율이 좋고, render/texture 가능. 단점은 생성이 어렵다는 것이다.

# PointNet

PointNet 자세한 논문리뷰는 [여기](https://younghyun197.github.io/paper_review/PointNet/)에서 확인가능하다. 

![](/assets/image/2022-05-25-18-38-19.png)


PointNet은 Input 으로 Point Cloud (2D arary)를 사용한다. 

output은 Classification, Part Segmentation, Semantic Segmentation 이다.

Point Cloud는 N개의 **Unordered** points로 D차원 vector를 나타낸다.

![](/assets/image/2022-05-25-18-41-03.png)

우리는 여기서 다음 2가지를 주의깊게 봐야한다.

1. Permutation Invariance / Equivariance
2. Transformation Invariance

# Permutation Invariance vs Equivariance

먼저 Permutation invariance를 살펴보자.

이것은 어떤 순서로 들어오더라도 그 특성이 변화하면 안되는 것을 의미한다.

즉, N개의 input이 있을때 N!개의 조합이 가능하고 어떻게 들어오더라도 같은 결과를 내야한다.

invariance는 Classification output을 도출할 때 사용된다. 

$$
f(x_1, x_2, ... , x_n) = y
$$


$$
f(x_{\pi_1}, x_{\pi_2}, ... , x_{\pi_n}) = y
$$


Permutation Equivarince

$$
f(x_1, x_2, ... , x_n) = (y_1, y_2, ..., y_n)
$$


$$
f(x_{\pi_1}, x_{\pi_2}, ... , x_{\pi_n}) = (y_1, y_2, ..., y_n)
$$

Equivariance는 Segmentation output을 도출할 때 사용된다. 

즉, 입력 순서와 상관 없이 동일한 출력을 얻기 위해서는 

Symmetric Function을 이용 할 수 있다.

$$
f(x_1, x_2, ... , x_n) \equiv f(x_{\pi_1}, x_{\pi_2}, ... , x_{\pi_n})
$$

그 예로 max함수 혹은 모든 노드를 덧셈하는 함수를 사용할 수 있다. 

![](/assets/image/2022-05-25-18-50-00.png)

![](/assets/image/2022-05-25-20-15-06.png)

h는 MLP를, g는 Max Pooling, r는 MLP를 의미한다. 

즉, MLP에 unorderd input을 넘긴뒤, Symmetric function인 MAX Pooling을 취해주고, 

해당 값을 다시 MLP에 통과시킴으로써, Permutation Equivariance를 유지한다.

# Transformation invariance 

Spatial Transformer의 아이디어는 

**Data dependent transformation for automatic alignment**이다. 

(자동정렬을 위한 데이터 종속변환)

즉, **input에 어떤 transformation이 가해져도 output에 영향을 끼치지 않도록 하는 방법**이다. 

![](/assets/image/2022-05-25-20-20-23.png)

point cloud를 입력으로 받아서 변환하는것이다. 

이때, input data에 대해 transformation matrix를 곱하는데, 

input transform에서는 3x3 transform을 진행하고,

feature transform에서는 64x64 matrix를 계산하는 것이 복잡하므로 

규제항을 추가해준다.

![](/assets/image/2022-05-25-20-23-48.png)


Spatil transformer를  통해 embedding space에서 학습된 latent feautres를 

변환하는 것이다. 

이렇게 총 2번의 변환을 거친뒤  MLP와 max pooling을 하면 global feature를 구할 수 있다. 

그렇다면 여기서 두 가지 질문을 할 수 있는데,

Q. 우리는 어떻게 per-point features를 구할 수 있을까?
    > A. 2번의 transform 이후 나오는 n x 64 output를 통해서 구할 수 있음. 

Q. 우리는 어떻게 global and local information을 합친 per-point features를 구할 수 있을까?
    > A. local and global feature를 Concatenate한 뒤에 MLP에서 processing한다. 


해당 논문에서 Classification은 어떤 방법으로 실험하였는지 알아보자.

우선 ModelNet Dataset을 사용하였는데, 12,311 CAD Models를 각각 3:1 비율로 train, test data로 사용하였다. 

총 40개의 클래스를 두었다. 

실험결과를 보면 기존의 3D CNN은 약 27000개의 파라미터가 필요했으나, 

PointNet에서는 3072개의 파라미터가 필요했다. 

Segmentation에서는 ShapeNet Dataset을 이용하였는데,

총 16,881 shapes를 가지고 16 개의 클래스를 갖는다.

각각의 클래스는 2-5 part labels를 갖는다. 

![](/assets/image/2022-05-25-20-35-30.png)

# Efiiciency of PointNet

![](/assets/image/2022-05-25-20-36-22.png)

기존의 방식대로는 point clouds를 voxel 혹은 polyhon으로 가공후 사용하였으나,

가공하지 않고 raw point clouds를 input으로 바로 사용하며 

연산량을 급격히 감소시켰다. 

# Limitation of PointNet

3D CNN의 경우 Hierachical feature learning에서 다양한 level의 abstraction을 갖는다. 

하지만 PointNet의 경우 Global feature learning에서는 하나의 point 혹은 모든 points의 특징을 학습한다.

즉, local contexts for points가 없다. 

![](/assets/image/2022-05-25-20-38-10.png)

PointNet은 사실 translation invariant 속성을 만족시키지 못한다. 

![](/assets/image/2022-05-25-20-38-24.png)

요약하면 설계상 PointNet은 metric space points에 의해 유도된 

local structures를 캡처하지 않으므로, fine-grained patterns과 

generalizability를 complex scenes으로 인식하는 기능이 제한된다.

(local feature는 Max Pooling을 이용하므로 안쓰이는 점이 많아 적게 반영된다.)